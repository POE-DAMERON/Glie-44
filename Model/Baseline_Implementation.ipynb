{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glie_44.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POE-DAMERON/Glie-44/blob/main/Model/Baseline_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPx23mf0avpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8278671-095d-4761-ccc8-18f3ccf8b50b"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from os import path, listdir\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import io\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "  Downloads Data from the VisDrone dataset.\n",
        "  Input is which dataset to download:\n",
        "    - 1 is the developper testing dataset\n",
        "    - 2 is the actual challenge testing dataset\n",
        "    - 3 is the val dataset\n",
        "    - Otherwise, the training dataset is extracted\n",
        "'''\n",
        "\n",
        "def initialize_training(file = 0):\n",
        "  !git clone https://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@github.com/POE-DAMERON/Glie-44.git\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  \n",
        "  if file == 1:\n",
        "    !unzip /content/drive/MyDrive/Glie_44/VisDrone2019-MOT-test-dev.zip\n",
        "  elif file == 2:\n",
        "    !unzip /content/drive/MyDrive/Glie_44/VisDrone2019-MOT-test-challenge.zip\n",
        "  elif file == 3:\n",
        "    !unzip /content/drive/MyDrive/Glie_44/VisDrone2019-MOT-val.zip\n",
        "  else:\n",
        "    !unzip /content/drive/MyDrive/Glie_44/VisDrone2019-MOT-train.zip\n",
        "\n",
        "initialize_training()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Glie-44'...\n",
            "remote: Enumerating objects: 2036, done.\u001b[K\n",
            "remote: Counting objects: 100% (467/467), done.\u001b[K\n",
            "remote: Compressing objects: 100% (405/405), done.\u001b[K\n",
            "Receiving objects: 100% (2036/2036), 336.31 MiB | 37.00 MiB/s, done.\n",
            "remote: Total 2036 (delta 221), reused 138 (delta 48), pack-reused 1569\u001b[K\n",
            "Resolving deltas: 100% (239/239), done.\n",
            "Checking out files: 100% (1516/1516), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWgZe8DQZS4"
      },
      "source": [
        "**New Model using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EcvQ0gWTmO"
      },
      "source": [
        "Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saxy2LvOmdbn"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOtVOkjWSps"
      },
      "source": [
        "import torch\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import os\n",
        "import pandas as pd\n",
        "import transforms as T\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "  get_results takes the result of the evaluate function as input\n",
        "  and returns the results as a String.\n",
        "\"\"\"\n",
        "\n",
        "def get_results(evaluator):\n",
        "\n",
        "  # Changes the system's output to a custom buffer\n",
        "\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "  # Outputs the result of the evaluation into the buffer\n",
        "\n",
        "  result = str(evaluator.coco_eval)\n",
        "  test = evaluator.coco_eval.items()\n",
        "  for iou_type, coco_eval in test:\n",
        "    print(\"IoU metric: {}\".format(iou_type))\n",
        "    try:\n",
        "      print(coco_eval)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Switches back to the system's output\n",
        "\n",
        "  sys.stdout = old_stdout\n",
        "  return buffer.getvalue()\n",
        "\n",
        "\"\"\"\n",
        "  Adds the arguments and results of a training session into a csv file\n",
        "  The inputs are:\n",
        "  a dictionnary of arguments,\n",
        "  the results as a String (with get_results for instance),\n",
        "  is_saved specifies whether the model was saved or not,\n",
        "  a path to save the model,\n",
        "  a path to save the records.\n",
        "\"\"\"\n",
        "\n",
        "def add_to_record(arguments, output, is_saved = False, path_to_saved_model = '', filepath = 'drive/MyDrive/Glie_44/training.csv'):\n",
        "  with open(filepath, 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([is_saved,\n",
        "                     path_to_saved_model,\n",
        "                     arguments['number_of_epochs'],\n",
        "                     arguments['batch_size'],\n",
        "                     arguments['optimizer'],\n",
        "                     arguments['lr'],\n",
        "                     arguments['weight_decay'],\n",
        "                     arguments['momentum'],\n",
        "                     arguments['lr_scheduler_step_size'],\n",
        "                     arguments['lr_scheduler_gamma'],\n",
        "                     output])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Saves the model and adds to a given csv record.\n",
        "  The inputs are:\n",
        "  the model,\n",
        "  the results as a String (with get_results for instance),\n",
        "  a dictionnary of the arguments,\n",
        "  a path to save the model,\n",
        "  a path to save the records.\n",
        "\"\"\"\n",
        "\n",
        "def save_model(model, evaluator, arguments, path = '', path_to_record = 'drive/MyDrive/Glie_44/training.csv'):\n",
        "  if path == '' or path == None:\n",
        "    path = 'drive/MyDrive/Glie_44/Models/model-' + str(int(time.time())) + '.pth'\n",
        "  torch.save(model,path) \n",
        "  add_to_record(arguments = arguments, output = evaluator, is_saved = True, path_to_saved_model = path, filename = path_to_record)\n",
        "\n",
        "\"\"\"\n",
        "  Loads the model from a specified path.\n",
        "  Takes the path as an input and returns the loaded model.\n",
        "\"\"\"\n",
        "\n",
        "def load_model(path):\n",
        "  model = torch.load(path)\n",
        "  return model\n",
        "\n",
        "\"\"\"\n",
        "  Saves the checkpoints in a given text file.\n",
        "  The inputs are:\n",
        "  a dictionnary of the arguments,\n",
        "  a path to save the checkpoints.\n",
        "\"\"\"\n",
        "\n",
        "def save_checkpoints(dicti, path = 'drive/MyDrive/Glie_44/Checkpoints/checkpoint.txt'):\n",
        "  with open(path, 'w') as f:\n",
        "    f.write(json.dumps(dicti))\n",
        "\n",
        "\"\"\"\n",
        "  Loads the checkpoints from a given text file.\n",
        "  Takes the path of the text file as input and returns a dictionnary of the\n",
        "  checkpoints.\n",
        "\"\"\"\n",
        "\n",
        "def load_checkpoints(path = 'drive/MyDrive/Glie_44/Checkpoints/checkpoint.txt'):\n",
        "  dicti = {}\n",
        "  with open(path, 'r') as f:\n",
        "    dicti = json.loads(f.read())\n",
        "  return dicti\n",
        "\n",
        "\"\"\"\n",
        "  Builds the dictionnary of the arguments based on the inputs that are:\n",
        "  the percentage of data trained on (float),\n",
        "  the number of epochs (int),\n",
        "  the batch_size (int),\n",
        "  the optimizer (sgd or adam usually)\n",
        "  learning rate (float),\n",
        "  momentum (float ,only for sgd),\n",
        "  weight decay (float),\n",
        "  step size (int),\n",
        "  gamma (float).\n",
        "\n",
        "  Returns a dictionnary.\n",
        "\"\"\"\n",
        "def get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma):\n",
        "  \n",
        "  arguments = {}\n",
        "\n",
        "  arguments['train_percentage'] = train_percentage\n",
        "  if (optimizer == 'adam'):\n",
        "    arguments['optimizer'] = optimizer\n",
        "    arguments['momentum'] = ''\n",
        "  else:\n",
        "    arguments['optimizer'] = 'sgd'\n",
        "    arguments['momentum'] = momentum\n",
        "  arguments['lr'] = lr\n",
        "  arguments['weight_decay'] = weight_decay\n",
        "  arguments['lr_scheduler_step_size'] = step_size\n",
        "  arguments['lr_scheduler_gamma'] = gamma\n",
        "  arguments['number_of_epochs'] = epochs\n",
        "  arguments['batch_size'] = batch_size\n",
        "\n",
        "  return arguments\n",
        "\n",
        "\"\"\"\n",
        "  Trains the model and returns a tuple composed of the model,\n",
        "  the results (String) and the arguments (dictionnary).\n",
        "\n",
        "  The path_to_the_saved_model is '' by default. This builds a new model from \n",
        "  scratch. path_to_the_saved_model will allow the function to load a custom\n",
        "  model.\n",
        "  \n",
        "  train_percentage is a float between 0 and 1 that determines the ratio of\n",
        "  trained images of the original dataset. Default value is 0.8.\n",
        "\n",
        "  test_percentage is a float between 0 and 1 that determines the ratio of\n",
        "  test images of the original dataset. It must be smaller than \n",
        "  (1 - train_percentage).\n",
        "  It can also be -1 in which cas it will be equal to (1 - train_percentage).\n",
        "  Default value is -1.\n",
        "\n",
        "  batch_size is a positive integer. Default value is 2.\n",
        "\n",
        "  epochs is a positive integer representing the total number of epochs before\n",
        "  ending the function. Default value is 10.\n",
        "\n",
        "  cur_epoch is a positive integer to start the training session from a given\n",
        "  epoch. Default value is 0.\n",
        "\n",
        "  optimizer is a string between sgd and adam representing the optimizer function\n",
        "  used. Default value is sgd.\n",
        "\n",
        "  lr is a positive float representing the learning rate related to the\n",
        "  optimizer. Default value is 0.005.\n",
        "\n",
        "  momentum is a float between 0 and 1 only used for the sgd optimizer. Default\n",
        "  value is 0.9.\n",
        "\n",
        "  weight_decay is a positive float related to the optimizer.\n",
        "  Default value is 0.0005.\n",
        "\n",
        "  step_size is a positive integer related to the learning rate scheduler.\n",
        "  Default value is 3.\n",
        "\n",
        "  gamma is a positive float related to the learning rate scheduler.\n",
        "  Default value is 0.1.\n",
        "\n",
        "  checkpoints is a positive integer representing the saving rate. If checkpoints\n",
        "  is 3, the function will save the model and the checkpoints every 3 epochs.\n",
        "  Default value is -1 which disables the autosave of the model and the\n",
        "  checkpoints.\n",
        "\n",
        "  load_checkpoint is a boolean that determines whether to load the checkpoints\n",
        "  or not. Default value is False.\n",
        "\n",
        "  output_path_for_model is the path to save the model. Default value is '' which\n",
        "  saves it on the same file it was loaded from.\n",
        "\n",
        "  checkpoint_path is the path to save the checkpoints. Default value is the path\n",
        "  used by the team during the initial training.\n",
        "\"\"\"\n",
        "\n",
        "def train(path_to_saved_model = '', train_percentage = .8, test_percentage = -1,\n",
        "          batch_size = 2, epochs = 10, cur_epoch=0, optimizer='sgd',\n",
        "          lr = 0.005, momentum = 0.9, weight_decay= 0.0005, step_size = 3,\n",
        "          gamma = 0.1, checkpoints=-1, load_checkpoint=False,\n",
        "          output_path_for_model = '',\n",
        "          checkpoint_path = 'drive/MyDrive/Glie_44/Checkpoints/checkpoint.txt',\n",
        "          preprocessing = Utils.to_tensor()):\n",
        "\n",
        "  \"\"\"\n",
        "    Prepares the variables before running the epochs\n",
        "  \"\"\"\n",
        "\n",
        "  # Creates the argument dictionnary to be returned\n",
        "\n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  # Checks for an available GPU, choses the CPU if none is found\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "  # Initializes the total dataset\n",
        "\n",
        "  X = AllVisDroneVideos(\n",
        "      Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"),\n",
        "      Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"),\n",
        "      preprocessing)\n",
        "\n",
        "  # Loads or builds the model, then links the model to the available device\n",
        "\n",
        "  if str(path_to_saved_model) == '' or str(path_to_saved_model) == None:\n",
        "    model = build_model()\n",
        "  else:\n",
        "    model = load_model(path_to_saved_model)\n",
        "  model.to(device)\n",
        "\n",
        "  # Checks for checkpoints to load\n",
        "\n",
        "  if load_checkpoint:\n",
        "    checkpoint = load_checkpoints()\n",
        "    gamma = checkpoint['gamma']\n",
        "    cur_epoch = checkpoint['cur_epoch']\n",
        "\n",
        "  # Builds the optimizers and learning rate scheduler required for training\n",
        "\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "  if (optimizer == 'adam'):\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optim = torch.optim.SGD(params, lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                    step_size=step_size,\n",
        "                                                    gamma=gamma)\n",
        "\n",
        "  \"\"\"\n",
        "    Runs the epochs\n",
        "  \"\"\"\n",
        "\n",
        "  for epoch in range(cur_epoch,epochs):\n",
        "    \n",
        "    \n",
        "    # Shuffles the data every epoch\n",
        "\n",
        "    random.shuffle(X.imgs)\n",
        "\n",
        "    # Prepares the training and testing sizes\n",
        "\n",
        "    train_sz = int(len(X) * train_percentage)\n",
        "    \n",
        "    if test_percentage == -1:\n",
        "        test_sz = len(X) - train_sz\n",
        "    else:\n",
        "        test_sz = int(len(X) * min(1,max(0, test_percentage)))\n",
        "\n",
        "    # Divides the shuffled dataset into training and testing sets\n",
        "\n",
        "    x_train = AllVisDroneVideos(\n",
        "        Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"),\n",
        "        Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"),\n",
        "        preprocessing,\n",
        "        X.imgs[:train_sz])\n",
        "    x_test = AllVisDroneVideos(\n",
        "        Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"),\n",
        "        Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"),\n",
        "        preprocessing,\n",
        "        X.imgs[train_sz:train_sz + test_sz])\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "      \n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "    print('\\n---------\\nTRAIN ONE EPOCH FINISHED\\n')\n",
        "\n",
        "    # Updates the learning rate scheduler\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Calling evaluate outputs the results so no need to print\n",
        "\n",
        "    results = get_results(evaluate(model, data_loader_test, device=device))\n",
        "\n",
        "    # Checks if the model and checkpoints should be saved\n",
        "\n",
        "    if checkpoints != -1 and epoch % checkpoints == 0:\n",
        "      if output_path_for_model !=  '':\n",
        "        save_model(model, results, arguments, output_path_for_model)\n",
        "      else:\n",
        "        save_model(model, results, arguments, path_to_saved_model)\n",
        "      save_checkpoints({'cur_epoch': epoch + 1, 'gamma': gamma}, path = checkpoint_path)\n",
        "      print('\\nModel Saved\\n')\n",
        "\n",
        "  return model, results, arguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyCvAuqO12L1"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "def build_model(num_classes = 12):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLmxWKCkRRan"
      },
      "source": [
        "\"\"\"\n",
        "  VisDroneVideo represents one video from the VisDrone dataset.\n",
        "  Images and targets can be accessed in a list-like manner.\n",
        "\"\"\"\n",
        "\n",
        "class VisDroneVideo(object):\n",
        "\n",
        "    \"\"\"\n",
        "      The inputs are:\n",
        "\n",
        "      root is the path of the directory with the images composing the video,\n",
        "\n",
        "      target_path is the path to the file with the video targets (boxes),\n",
        "\n",
        "      preprocessing is a transform function useful for data augmentation, it is\n",
        "      advised to work with torchvision's transforms libary,\n",
        "\n",
        "      imgs is a custom list of image path instead of using the entireity of the\n",
        "      images composing the directory,\n",
        "\n",
        "      include_targets is a boolean to include targets or not, in case no\n",
        "      training is executed.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, target_path, preprocessing = None, imgs = None, include_targets = True):\n",
        "        self.root = str(root)\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        if (imgs != None):\n",
        "          self.imgs = imgs\n",
        "        else:\n",
        "          # Adds the sorted images in the class' attribute\n",
        "          self.imgs = sorted(listdir(Path(root)), key=lambda x: x.lstrip(\"_\"))\n",
        "\n",
        "        self._include_targets = include_targets\n",
        "        if include_targets:\n",
        "          self.target = target_path\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "      Retrieves the prepared image and respective targets in a list-like manner.\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Loads image\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Creates the targets' dictionnary if required\n",
        "\n",
        "        if self._include_targets:\n",
        "          video_targets = Utils.read_txt_visdrone(self.target)\n",
        "          image_targets = self.clean_targets(video_targets, idx)\n",
        "          image_targets[\"is_crowd\"] = 0\n",
        "          boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "          boxes = boxes.astype('float32')\n",
        "\n",
        "          boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "          labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "          crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "          image_id = torch.tensor([idx])\n",
        "          area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "          target = {}\n",
        "          target[\"boxes\"] = boxes\n",
        "          target[\"labels\"] = labels\n",
        "          target[\"image_id\"] = image_id\n",
        "          target[\"area\"] = area\n",
        "          target[\"iscrowd\"] = crowd\n",
        "\n",
        "          if self.preprocessing is not None:\n",
        "              img = self.preprocessing(img)\n",
        "\n",
        "          return img, target\n",
        "        \n",
        "        else:\n",
        "          img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "          img = Image.open(img_path).convert(\"RGB\")\n",
        "          if self.preprocessing is not None:\n",
        "              img = self.preprocessing(img)\n",
        "\n",
        "          return [img]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    \"\"\"\n",
        "      Modifies the data to match the model. Returns the modified targets'\n",
        "      dictionnary.\n",
        "    \"\"\"\n",
        "\n",
        "    def clean_targets(self, targets, idx):\n",
        "\n",
        "        targets = self.targetsToDataframe(targets)\n",
        "        targets = targets[(targets.object_category != \"0\")]\n",
        "        targets = targets[(targets.frame_index == str(idx+1))]\n",
        "\n",
        "        targets.bbox_top = targets.bbox_top.astype('float32')\n",
        "        targets.bbox_height = targets.bbox_height.astype('float32')\n",
        "        targets.bbox_left = targets.bbox_left.astype('float32')\n",
        "        targets.bbox_width = targets.bbox_width.astype('float32')\n",
        "\n",
        "        targets[\"bottom\"] = targets.bbox_top + targets.bbox_height\n",
        "        targets[\"right\"] = targets.bbox_left + targets.bbox_width\n",
        "\n",
        "        return targets\n",
        "\n",
        "    \"\"\"\n",
        "      Returns a dataframe from the given targets.\n",
        "    \"\"\"\n",
        "    \n",
        "    def targetsToDataframe(self, array):\n",
        "        columns = [\n",
        "          \"frame_index\",\n",
        "          \"target_id\",\n",
        "          \"bbox_left\",\n",
        "          \"bbox_top\",\n",
        "          \"bbox_width\",\n",
        "          \"bbox_height\",\n",
        "          \"score\",\n",
        "          \"object_category\",\n",
        "          \"truncation\",\n",
        "          \"oclusion\"\n",
        "        ]\n",
        "        return pd.DataFrame(data=array,columns=columns)\n",
        "\n",
        "\"\"\"\n",
        "  VisDroneDataset represents all the videos from the VisDrone dataset.\n",
        "  Each VisDroneVideos can be accessed in a list-like manner.\n",
        "\"\"\"\n",
        "\n",
        "class VisDroneDataset(object):\n",
        "    \"\"\"\n",
        "      The inputs are:\n",
        "\n",
        "      root is the path of the directory with the video directories,\n",
        "\n",
        "      preprocessing is a transform function useful for data augmentation, it is\n",
        "      advised to work with torchvision's transforms libary,\n",
        "\n",
        "      include_targets is a boolean to include targets or not, in case no\n",
        "      training is executed.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, preprocessing = None, include_targets = True):\n",
        "        self.root = root\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        self.videos = sorted(listdir(Path(root).joinpath(\"sequences\")),\n",
        "                             key=lambda x: x.lstrip(\"_\"))\n",
        "        self._include_targets = include_targets\n",
        "        if include_targets:\n",
        "          self.targets = sorted(listdir(Path(root).joinpath(\"annotations\")),\n",
        "                                key=lambda x: x.lstrip(\"_\"))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      if self._include_targets:\n",
        "        return VisDroneVideo(\n",
        "            Path(self.root).joinpath(\"sequences\").joinpath(self.videos[idx]),\n",
        "            Path(self.root).joinpath(\"annotations\").joinpath(self.targets[idx]),\n",
        "            self.preprocessing)\n",
        "      else:\n",
        "        return VisDroneVideo(\n",
        "            Path(self.root).joinpath(\"sequences\").joinpath(self.videos[idx]),\n",
        "            None,\n",
        "            self.preprocessing, \n",
        "            include_targets=self._include_targets)\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    def get_video_path(self,video_index):\n",
        "      return str(Path(self.root).joinpath(\"sequences\").joinpath(self.videos[video_index]))\n",
        "\n",
        "    def get_image_path(self, video_index, image_index):\n",
        "      return str(Path(self[video_index].root).joinpath(self[video_index].imgs[image_index]))\n",
        "\n",
        "    def get_image(self, video_index, image_index):\n",
        "      return Image.open(self.get_image_path(video_index, image_index))\n",
        "\n",
        "\"\"\"\n",
        "  AllVisDroneVideos is a subclass of the VisDroneVideos related to the new \n",
        "  architecture of the training dataset composed of only one folder with all the \n",
        "  images from each video.\n",
        "\"\"\"\n",
        "\n",
        "class AllVisDroneVideos(VisDroneVideo):\n",
        "    \"\"\"\n",
        "      The inputs are:\n",
        "\n",
        "      root is the path of the directory with the images,\n",
        "\n",
        "      targets_path is the path to the directory with the targets (boxes),\n",
        "\n",
        "      preprocessing is a transform function useful for data augmentation, it is\n",
        "      advised to work with torchvision's transforms libary,\n",
        "\n",
        "      imgs is a custom list of image path instead of using the entireity of the\n",
        "      images composing the directory.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,root, targets_path, preprocessing = None, imgs = None):\n",
        "      super().__init__(root, targets_path, preprocessing, imgs)\n",
        "      self.targets = sorted(listdir(Path(targets_path)),\n",
        "                            key=lambda x: x.lstrip(\"_\"))\n",
        "\n",
        "    def __len__(self):\n",
        "      return super().__len__()\n",
        "\n",
        "    \"\"\"\n",
        "      Returns the video name corresponding to an image path and the image index.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_video_name(self, image_path):\n",
        "      return image_path.stem[:-8] + \".txt\", int(image_path.stem[-7:]) -1\n",
        "\n",
        "    \"\"\"\n",
        "      Retrieves the prepared image and respective targets in a list-like manner.\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_name, image_idx = self.get_video_name(img_path)\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(Path(self.target).joinpath(video_name))\n",
        "        image_targets = self.clean_targets(video_targets, image_idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(\n",
        "            image_targets.object_category.astype('int64').values,\n",
        "            dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values,\n",
        "                                dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img = self.preprocessing(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    \"\"\"\n",
        "      Returns the video index from a given image index.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_video_from_idx(self,idx):\n",
        "\n",
        "      video_lengths = [269, 58, 118, 501, 181, 85, 217, 97, 361, 361,\n",
        " 516, 1255, 398, 412, 213, 256, 261, 307, 348, 225, 421, 680, 341, 768, 721,\n",
        " 677, 725, 616, 548, 116, 680, 872, 962, 547, 508, 1424, 500, 210, 346, 556, \n",
        " 414, 230, 185, 403, 632, 127, 426, 369, 196, 277, 196, 691, 421, 219, 462,296]\n",
        "      \n",
        "      for i in range(len(video_lengths)):\n",
        "        if idx<video_lengths[i]:\n",
        "          return i, idx\n",
        "        else:\n",
        "          idx -= video_lengths[i]\n",
        "      return len(video_lengths), idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XhaZ_CZ6Z1"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKM3TFaZ6M8"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  \"\"\"\n",
        "    converts a PIL image into a PyTorch Tensor\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def to_tensor():\n",
        "    return torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "  \"\"\"\n",
        "    Input: Path of the txt file with annotations as in the VisDrone dataset\\n\n",
        "    Output: A numpy array containing the information for bounding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def read_txt_visdrone(path):\n",
        "      lines = []\n",
        "      with open(path) as f:\n",
        "          lines = f.readlines()\n",
        "          f.close()\n",
        "      df = []\n",
        "      for x in lines:\n",
        "          splitLine = x.split(\",\")\n",
        "          splitLine[-1] = splitLine[-1].split(\"\\n\")[0]\n",
        "          df.append(splitLine)\n",
        "      return df\n",
        "  \n",
        "  \"\"\"\n",
        "    Adds boxes to a PIL image and takes as inputs:\n",
        "    the PIL image,\n",
        "    an array of boxes,\n",
        "    a list of classes,\n",
        "    a list of scores,\n",
        "    a path to the font to use,\n",
        "    the precision as a float between 0 and 1.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def add_blocks(image, boxes, classes, scores, font_path, precision):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    width, height = image.size\n",
        "    for i in range(len(boxes)):\n",
        "      if scores[i] > precision and classes[i] != 0:\n",
        "        draw.rectangle(boxes[i], outline = Utils.which_color(classes[i]), width = 3)\n",
        "        draw.text((boxes[i][0], boxes[i][1]),Utils.box_title(classes[i]), fill=(255,255,255), stroke_fill= (0,0,0,255), stroke_width = 2, font= ImageFont.truetype(font_path, 20))\n",
        "\n",
        "  \"\"\"\n",
        "    Returns the text to add on the screen to identify the class.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def box_title(label_index):\n",
        "    return str(label_index)\n",
        "\n",
        "  \"\"\"\n",
        "    Returns a tuple representing the RGB colors depending on the class.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def which_color(class_id):\n",
        "    color_value = int(class_id) * 64\n",
        "    return (min(color_value, 255), max(min(color_value - 255, 255),0),max(min(color_value - 256 * 2 - 1, 255),0))\n",
        "\n",
        "  @staticmethod\n",
        "  def prepare_coords(array):\n",
        "    return (array[1], array[0], array[3], array[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}