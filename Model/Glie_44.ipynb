{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glie_44.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POE-DAMERON/Glie-44/blob/main/Model/Glie_44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPx23mf0avpR"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from os import path, listdir\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import io\n",
        "import random\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "  Downloads Data from the VisDrone dataset.\n",
        "  Input is which dataset to download:\n",
        "    - 0 is the training dataset\n",
        "    - 1 is the developper testing dataset\n",
        "    - 2 is the actual challenge testing dataset\n",
        "    - 3 is the val dataset\n",
        "'''\n",
        "\n",
        "def initialize_training(file = 0):\n",
        "  !git clone https://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@github.com/POE-DAMERON/Glie-44.git\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  \n",
        "  if file == 1:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-dev.zip\n",
        "  elif file == 2:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-challenge.zip\n",
        "  elif file == 3:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-val.zip\n",
        "  else:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-train.zip\n",
        "\n",
        "#initialize_training()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWgZe8DQZS4"
      },
      "source": [
        "**New Model using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EcvQ0gWTmO"
      },
      "source": [
        "Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saxy2LvOmdbn",
        "outputId": "f3a14def-ec9b-4be3-864b-9e4bc5750424"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 22445, done.\u001b[K\n",
            "remote: Counting objects: 100% (3226/3226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (803/803), done.\u001b[K\n",
            "remote: Total 22445 (delta 2471), reused 3037 (delta 2343), pack-reused 19219\u001b[K\n",
            "Receiving objects: 100% (22445/22445), 27.08 MiB | 26.94 MiB/s, done.\n",
            "Resolving deltas: 100% (16530/16530), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOtVOkjWSps"
      },
      "source": [
        "import torch\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import os\n",
        "import pandas as pd\n",
        "import transforms as T\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def get_results(evaluator):\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "  result = str(evaluator.coco_eval)\n",
        "  test = evaluator.coco_eval.items()\n",
        "  for iou_type, coco_eval in test:\n",
        "    print(\"IoU metric: {}\".format(iou_type))\n",
        "    try:\n",
        "      print(coco_eval)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  sys.stdout = old_stdout\n",
        "\n",
        "  return buffer.getvalue()\n",
        "\n",
        "def add_to_record(arguments, output, is_saved = False, path_to_saved_model = ''):\n",
        "\n",
        "  with open('drive/MyDrive/training.csv', 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    #writer.writerow(['Saved', 'path_to_saved_model', 'number_of_epochs', 'batch_size', 'optimizer', 'learning_rate', 'weight_decay', 'momentum', 'lr_scheduler_step_size', 'lr_scheduler_gamma', 'output'])\n",
        "    writer.writerow([is_saved, path_to_saved_model, arguments['number_of_epochs'], arguments['batch_size'], arguments['optimizer'], arguments['lr'], arguments['weight_decay'], arguments['momentum'], arguments['lr_scheduler_step_size'], arguments['lr_scheduler_gamma'], output])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  saving the model will add it to the record\n",
        "\"\"\"\n",
        "\n",
        "def save_model(model, evaluator, arguments, path = ''):\n",
        "\n",
        "  \"\"\"\n",
        "      saves the model and adds to the csv records\n",
        "\n",
        "      2 methods :\n",
        "        - saving the whole model\n",
        "        - saving the weights and info\n",
        "  \"\"\"\n",
        "\n",
        "  if path == '':\n",
        "    path = 'drive/MyDrive/Models/model-' + str(int(time.time())) + '.pth'\n",
        "\n",
        "  torch.save(model,path) \n",
        "\n",
        "  #torch.save(model.state_dict(), path)\n",
        "  add_to_record(arguments = arguments, output = get_results(evaluator), is_saved = True, path_to_saved_model = path)\n",
        "  \n",
        "def load_model(path):\n",
        "\n",
        "  model = torch.load(path)\n",
        "\n",
        "  \"\"\"\n",
        "    model = Model()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "  \"\"\"\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma):\n",
        "  \n",
        "  arguments = {}\n",
        "\n",
        "  arguments['train_percentage'] = train_percentage\n",
        "  if (optimizer == 'adam'):\n",
        "    arguments['optimizer'] = optimizer\n",
        "    arguments['momentum'] = ''\n",
        "  else:\n",
        "    arguments['optimizer'] = 'sgd'\n",
        "    arguments['momentum'] = momentum\n",
        "  arguments['lr'] = lr\n",
        "  arguments['weight_decay'] = weight_decay\n",
        "  arguments['lr_scheduler_step_size'] = step_size\n",
        "  arguments['lr_scheduler_gamma'] = gamma\n",
        "  arguments['number_of_epochs'] = epochs\n",
        "  arguments['batch_size'] = batch_size\n",
        "\n",
        "  return arguments\n",
        "\n",
        "def train_videos(path_to_saved_model = '', train_percentage = .8, test_size = -1,\n",
        "          train_size = -1, batch_size = 2, epochs = 10, optimizer='sgd',\n",
        "          lr = 0.005, momentum = 0.9, weight_decay= 0.0005, step_size = 3,\n",
        "          gamma = 0.1):\n",
        "  \n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  x = VisDroneDataset(Path().absolute().joinpath('VisDrone2019-MOT-train'), Utils.to_tensor())\n",
        "  testing_data = torch.utils.data.DataLoader(x[0])\n",
        "\n",
        "  for i in range(len(x)):\n",
        "\n",
        "    print(\"\\n-----------------------------\\nVideo {} / {}\\n\".format(i+1, len(x)))\n",
        "\n",
        "    X = x[i]\n",
        "\n",
        "    if train_size == -1:\n",
        "      train_sz = int(len(X) * train_percentage)\n",
        "    else:\n",
        "      train_sz = train_size\n",
        "      \n",
        "    x_train, x_test = torch.utils.data.random_split(X, [train_sz, len(X) - train_sz])\n",
        "\n",
        "    if test_size != -1:\n",
        "      x_test = Utils.slice(x_test, 0, test_size)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "          x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "          collate_fn=utils.collate_fn)\n",
        "    \n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "      x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "    testing_data = data_loader_test\n",
        "    \n",
        "    if str(path_to_saved_model) == '':\n",
        "      model = build_model()\n",
        "    else:\n",
        "      model = load_model(path_to_saved_model)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    if (optimizer == 'adam'):\n",
        "      optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "      optim = torch.optim.SGD(params, lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                    step_size=step_size,\n",
        "                                                    gamma=gamma)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "          train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "          lr_scheduler.step()\n",
        "          evaluate(model, testing_data, device=device)\n",
        "  \n",
        "  return model, evaluate(model, testing_data, device=device), arguments\n",
        "\n",
        "def train(path_to_saved_model = '', train_percentage = .8, test_percentage = -1,\n",
        "          test_size = -1, train_size = -1, batch_size = 2, epochs = 10, optimizer='sgd',\n",
        "          lr = 0.005, momentum = 0.9, weight_decay= 0.0005, step_size = 3,\n",
        "          gamma = 0.1):\n",
        "  \n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  X = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor())\n",
        "\n",
        "  if train_size == -1:\n",
        "    train_sz = int(len(X) * train_percentage)\n",
        "  else:\n",
        "    train_sz = train_size\n",
        "  \n",
        "  test_sz = int(len(X) * min(1,max(0, test_percentage)))\n",
        "\n",
        "  x_train, x_test, x_overflow = torch.utils.data.random_split(X, [train_sz, test_sz , len(X) - train_sz - test_sz])\n",
        "\n",
        "  random.shuffle(X.imgs)\n",
        "\n",
        "  x_train = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor(), X.imgs[:train_sz])\n",
        "  x_test = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor(), X.imgs[train_sz:train_sz + test_sz])\n",
        "\n",
        "  if test_size != -1:\n",
        "    x_test = Utils.slice(x_test, 0, test_size)\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "      x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "  data_loader_test = torch.utils.data.DataLoader(\n",
        "      x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "  if str(path_to_saved_model) == '':\n",
        "    model = build_model()\n",
        "  else:\n",
        "    model = load_model(path_to_saved_model)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "  if (optimizer == 'adam'):\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optim = torch.optim.SGD(params, lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                    step_size=step_size,\n",
        "                                                    gamma=gamma)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "    print('\\n---------\\nTRAIN ONE EPOCH FINISHED')\n",
        "    lr_scheduler.step()\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "    results = get_results(evaluate(model, data_loader_test, device=device))\n",
        "    #print(results)\n",
        "  \n",
        "  return model, results, arguments\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyCvAuqO12L1"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "def build_model(num_classes = 12):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  \"\"\"\n",
        "  backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "  backbone.out_channels = 1280\n",
        "\n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                  output_size=7,\n",
        "                                                  sampling_ratio=2)\n",
        "  return FasterRCNN(backbone,\n",
        "                    num_classes=num_classes,\n",
        "                    rpn_anchor_generator=anchor_generator,\n",
        "                    box_roi_pool=roi_pooler)\n",
        "  \"\"\"\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLmxWKCkRRan"
      },
      "source": [
        "class VisDroneVideo(object):\n",
        "    def __init__(self ,root, target_path, preprocessing = None, imgs = None):\n",
        "        self.root = str(root)\n",
        "        self.preprocessing = preprocessing\n",
        "        if (imgs != None):\n",
        "          self.imgs = imgs\n",
        "        else:\n",
        "          self.imgs = sorted(listdir(Path(root)), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.target = target_path\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(self.target)\n",
        "        image_targets = self.clean_targets(video_targets, idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img, target = self.preprocessing(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def clean_targets(self, targets, idx):\n",
        "\n",
        "        targets = self.targetsToDataframe(targets)\n",
        "        targets = targets[(targets.object_category != \"0\")]\n",
        "        targets = targets[(targets.frame_index == str(idx+1))]\n",
        "\n",
        "        targets.bbox_top = targets.bbox_top.astype('float32')\n",
        "        targets.bbox_height = targets.bbox_height.astype('float32')\n",
        "        targets.bbox_left = targets.bbox_left.astype('float32')\n",
        "        targets.bbox_width = targets.bbox_width.astype('float32')\n",
        "\n",
        "        targets[\"bottom\"] = targets.bbox_top + targets.bbox_height\n",
        "        targets[\"right\"] = targets.bbox_left + targets.bbox_width\n",
        "\n",
        "        return targets\n",
        "    \n",
        "    def targetsToDataframe(self, array):\n",
        "        columns = [\n",
        "          \"frame_index\",\n",
        "          \"target_id\",\n",
        "          \"bbox_left\",\n",
        "          \"bbox_top\",\n",
        "          \"bbox_width\",\n",
        "          \"bbox_height\",\n",
        "          \"score\",\n",
        "          \"object_category\",\n",
        "          \"truncation\",\n",
        "          \"oclusion\"\n",
        "        ]\n",
        "        return pd.DataFrame(data=array,columns=columns)\n",
        "\n",
        "class AllVisDroneVideos(VisDroneVideo):\n",
        "    def __init__(self,root, targets_path, preprocessing = None, imgs = None):\n",
        "      super().__init__(root, targets_path, preprocessing, imgs)\n",
        "      self.targets = sorted(listdir(Path(targets_path)), key=lambda x: x.lstrip(\"_\"))\n",
        "\n",
        "    def __len__(self):\n",
        "      return super().__len__()\n",
        "\n",
        "    def get_video_name(self, image_path):\n",
        "      return image_path.stem[:-8] + \".txt\", int(image_path.stem[-7:]) -1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_name, image_idx = self.get_video_name(img_path)\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(Path(self.target).joinpath(video_name))\n",
        "        image_targets = self.clean_targets(video_targets, image_idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img, target = self.preprocessing(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def get_video_from_idx(self,idx):\n",
        "\n",
        "      video_lengths = [269, 58, 118, 501, 181, 85, 217, 97, 361, 361,\n",
        " 516, 1255, 398, 412, 213, 256, 261, 307, 348, 225, 421, 680, 341, 768, 721,\n",
        " 677, 725, 616, 548, 116, 680, 872, 962, 547, 508, 1424, 500, 210, 346, 556, \n",
        " 414, 230, 185, 403, 632, 127, 426, 369, 196, 277, 196, 691, 421, 219, 462,296]\n",
        "      \n",
        "      for i in range(len(video_lengths)):\n",
        "        if idx<video_lengths[i]:\n",
        "          return i, idx\n",
        "        else:\n",
        "          idx -= video_lengths[i]\n",
        "      return len(video_lengths), idx\n",
        "\n",
        "class VisDroneDataset(object):\n",
        "    def __init__(self, root, preprocessing = None):\n",
        "        self.root = root\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        self.videos = sorted(listdir(Path(root).joinpath(\"sequences\")), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.targets = sorted(listdir(Path(root).joinpath(\"annotations\")), key=lambda x: x.lstrip(\"_\"))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      return VisDroneVideo(Path(self.root).joinpath(\"sequences\").joinpath(self.videos[idx]),Path(self.root).joinpath(\"annotations\").joinpath(self.targets[idx]), self.preprocessing)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    '''\n",
        "        Concatenates the images of all videos into a large VisDroneVideo class\n",
        "        Allows the model to train on a broader sample of data\n",
        "    \n",
        "\n",
        "    def all_images(self):\n",
        "        large_dataset = VisDroneVideo()\n",
        "\n",
        "        for i in self:\n",
        "          for j in i[0]:\n",
        "            print(j)\n",
        "\n",
        "    '''"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XhaZ_CZ6Z1"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKM3TFaZ6M8"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  @staticmethod\n",
        "  def to_tensor():\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "  @staticmethod\n",
        "  def read_txt_visdrone(path):\n",
        "      \"\"\"\n",
        "          Input: Path of the txt file with annotations as in the VisDrone dataset\\n\n",
        "          Output: A numpy array containing the information for bounding boxes\n",
        "      \"\"\"\n",
        "      lines = []\n",
        "      with open(path) as f:\n",
        "          lines = f.readlines()\n",
        "          f.close()\n",
        "      df = []\n",
        "      for x in lines:\n",
        "          splitLine = x.split(\",\")\n",
        "          splitLine[-1] = splitLine[-1].split(\"\\n\")[0]\n",
        "          df.append(splitLine)\n",
        "      return df\n",
        "      \n",
        "  @staticmethod\n",
        "  def bottom_right(left,top,width,height):\n",
        "      \"\"\"\n",
        "          Input: co-ordinates for the top left corner of a bounding box and its width and height\\n\n",
        "          Output: co-ordinates for the bottom right corner\n",
        "      \"\"\"\n",
        "      bottom = top + height\n",
        "      right = left + width\n",
        "      return right, bottom\n",
        "\n",
        "  @staticmethod\n",
        "  def slice(array, start, stop):\n",
        "    result = []\n",
        "    for i in range(stop-start):\n",
        "      result.append(array[start + i])\n",
        "    return result"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn4oKgG2g3nu",
        "outputId": "8b390ad3-6b61-45c4-8669-8c77f6a9ffee"
      },
      "source": [
        "eval = train(epochs = 2, train_percentage= .01, test_percentage=.01, optimizer='adam', lr=.0005)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [  0/121]  eta: 0:03:10  lr: 0.000005  loss: 3.7144 (3.7144)  loss_classifier: 2.6172 (2.6172)  loss_box_reg: 0.5323 (0.5323)  loss_objectness: 0.4239 (0.4239)  loss_rpn_box_reg: 0.1410 (0.1410)  time: 1.5780  data: 0.8452  max mem: 9323\n",
            "Epoch: [0]  [ 10/121]  eta: 0:01:20  lr: 0.000046  loss: 2.9585 (3.2071)  loss_classifier: 2.1025 (2.0547)  loss_box_reg: 0.7149 (0.7197)  loss_objectness: 0.1302 (0.2946)  loss_rpn_box_reg: 0.1350 (0.1381)  time: 0.7251  data: 0.0916  max mem: 9323\n",
            "Epoch: [0]  [ 20/121]  eta: 0:01:09  lr: 0.000088  loss: 2.1334 (2.5591)  loss_classifier: 0.9713 (1.4776)  loss_box_reg: 0.7120 (0.7038)  loss_objectness: 0.1314 (0.2309)  loss_rpn_box_reg: 0.1401 (0.1469)  time: 0.6432  data: 0.0157  max mem: 9323\n",
            "Epoch: [0]  [ 30/121]  eta: 0:01:01  lr: 0.000130  loss: 1.6853 (2.2393)  loss_classifier: 0.7403 (1.2121)  loss_box_reg: 0.6993 (0.6885)  loss_objectness: 0.1314 (0.1971)  loss_rpn_box_reg: 0.1401 (0.1416)  time: 0.6509  data: 0.0163  max mem: 9323\n",
            "Epoch: [0]  [ 40/121]  eta: 0:00:54  lr: 0.000171  loss: 1.5634 (2.0715)  loss_classifier: 0.6110 (1.0698)  loss_box_reg: 0.6513 (0.6824)  loss_objectness: 0.1072 (0.1794)  loss_rpn_box_reg: 0.1262 (0.1399)  time: 0.6636  data: 0.0176  max mem: 9323\n",
            "Epoch: [0]  [ 50/121]  eta: 0:00:47  lr: 0.000213  loss: 1.4576 (1.9510)  loss_classifier: 0.5914 (0.9750)  loss_box_reg: 0.6082 (0.6679)  loss_objectness: 0.1247 (0.1702)  loss_rpn_box_reg: 0.1252 (0.1380)  time: 0.6704  data: 0.0163  max mem: 9323\n",
            "Epoch: [0]  [ 60/121]  eta: 0:00:41  lr: 0.000254  loss: 1.4421 (1.8488)  loss_classifier: 0.5799 (0.8977)  loss_box_reg: 0.5858 (0.6446)  loss_objectness: 0.1312 (0.1679)  loss_rpn_box_reg: 0.1252 (0.1386)  time: 0.6661  data: 0.0159  max mem: 9323\n",
            "Epoch: [0]  [ 70/121]  eta: 0:00:34  lr: 0.000296  loss: 1.5126 (1.8081)  loss_classifier: 0.6018 (0.8562)  loss_box_reg: 0.6157 (0.6423)  loss_objectness: 0.1501 (0.1694)  loss_rpn_box_reg: 0.1259 (0.1402)  time: 0.6586  data: 0.0172  max mem: 9323\n",
            "Epoch: [0]  [ 80/121]  eta: 0:00:27  lr: 0.000338  loss: 1.5197 (1.7856)  loss_classifier: 0.6062 (0.8238)  loss_box_reg: 0.6271 (0.6423)  loss_objectness: 0.1825 (0.1770)  loss_rpn_box_reg: 0.1465 (0.1426)  time: 0.6476  data: 0.0165  max mem: 9323\n",
            "Epoch: [0]  [ 90/121]  eta: 0:00:20  lr: 0.000379  loss: 1.5358 (1.7503)  loss_classifier: 0.5959 (0.7963)  loss_box_reg: 0.6077 (0.6291)  loss_objectness: 0.2029 (0.1809)  loss_rpn_box_reg: 0.1486 (0.1439)  time: 0.6390  data: 0.0151  max mem: 9323\n",
            "Epoch: [0]  [100/121]  eta: 0:00:13  lr: 0.000421  loss: 1.6755 (1.7538)  loss_classifier: 0.6094 (0.7822)  loss_box_reg: 0.5914 (0.6309)  loss_objectness: 0.2062 (0.1913)  loss_rpn_box_reg: 0.1632 (0.1494)  time: 0.6365  data: 0.0160  max mem: 9323\n",
            "Epoch: [0]  [110/121]  eta: 0:00:07  lr: 0.000463  loss: 1.6755 (1.7438)  loss_classifier: 0.6450 (0.7710)  loss_box_reg: 0.6149 (0.6303)  loss_objectness: 0.2313 (0.1923)  loss_rpn_box_reg: 0.1683 (0.1503)  time: 0.6348  data: 0.0164  max mem: 9323\n",
            "Epoch: [0]  [120/121]  eta: 0:00:00  lr: 0.000500  loss: 1.5568 (1.7297)  loss_classifier: 0.6118 (0.7577)  loss_box_reg: 0.6110 (0.6271)  loss_objectness: 0.1709 (0.1917)  loss_rpn_box_reg: 0.1654 (0.1532)  time: 0.6308  data: 0.0144  max mem: 9323\n",
            "Epoch: [0] Total time: 0:01:19 (0.6566 s / it)\n",
            "\n",
            "---------\n",
            "TRAIN ONE EPOCH FINISHED\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [  0/242]  eta: 0:03:30  model_time: 0.2048 (0.2048)  evaluator_time: 0.1343 (0.1343)  time: 0.8705  data: 0.5270  max mem: 9323\n",
            "Test:  [100/242]  eta: 0:00:32  model_time: 0.1311 (0.1337)  evaluator_time: 0.0469 (0.0640)  time: 0.2190  data: 0.0198  max mem: 9323\n",
            "Test:  [200/242]  eta: 0:00:09  model_time: 0.1314 (0.1345)  evaluator_time: 0.0591 (0.0745)  time: 0.2507  data: 0.0178  max mem: 9323\n",
            "Test:  [241/242]  eta: 0:00:00  model_time: 0.1303 (0.1342)  evaluator_time: 0.0421 (0.0736)  time: 0.2099  data: 0.0102  max mem: 9323\n",
            "Test: Total time: 0:00:57 (0.2356 s / it)\n",
            "Averaged stats: model_time: 0.1303 (0.1342)  evaluator_time: 0.0421 (0.0736)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.39s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.037\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.093\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.042\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.054\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.051\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.041\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.086\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiC4psA5EMAi"
      },
      "source": [
        "add_to_record(arguments = eval[2], output = eval[1],is_saved = False)\n",
        "#save_model(eval[0], eval[1], eval[2], 'model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCt_fJY74uI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5a300e-0145-4f4d-adfa-b7d4a8334316"
      },
      "source": [
        "print(eval[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}