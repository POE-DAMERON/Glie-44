{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glie_44.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POE-DAMERON/Glie-44/blob/Pytorch/Model/Glie_44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPx23mf0avpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba3d5e8-4cdc-4396-96bb-74659a33ee32"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from os import path, listdir\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!git clone https://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@github.com/POE-DAMERON/Glie-44.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Glie-44' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwwkaAuJavpY"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  @staticmethod\n",
        "  def add_blocks(pred, draw, height, width, font_path, precision):\n",
        "    boxes = pred['detection_boxes'].numpy()[0]\n",
        "    classes = pred['detection_classes'].numpy()[0]\n",
        "\n",
        "    boxes[:, 0] *= height\n",
        "    boxes[:, 1] *= width\n",
        "    boxes[:, 2] *= height\n",
        "    boxes[:, 3] *= width\n",
        "    \n",
        "    for i in range(len(boxes)):\n",
        "      if pred['detection_scores'].numpy()[0][i] > precision:\n",
        "        draw.rectangle(Utils.prepare_coords(boxes[i]), outline = Utils.which_color(classes[i]), width = 3)\n",
        "        draw.text((boxes[i][1], boxes[i][0]),str(classes[i])[:-2], fill=(255,255,255), stroke_fill= (0,0,0,255), stroke_width = 2, font= ImageFont.truetype(font_path, 20))\n",
        "\n",
        "  @staticmethod\n",
        "  def which_color(class_id):\n",
        "    color_value = int(class_id) * 9\n",
        "    return (min(color_value, 255), max(min(color_value - 255, 255),0),max(min(color_value - 256 * 2 - 1, 255),0))\n",
        "\n",
        "  @staticmethod\n",
        "  def prepare_coords(array):\n",
        "    return (array[1], array[0], array[3], array[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7cbjDkH6X9"
      },
      "source": [
        "class Glie_44():\n",
        "\n",
        "  def __init__(self, precision = .2, font_path = Path().absolute().joinpath('Glie-44/Model/Data/fonts/Roboto-Regular.ttf')):\n",
        "    self._precision = precision\n",
        "    self._font_path = str(font_path)\n",
        "\n",
        "\n",
        "  def set_font_path(self, font_path):\n",
        "    self._font_path = font_path\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "    self._model = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
        "\n",
        "  def set_model(self,model):\n",
        "    self._model = model\n",
        "\n",
        "  def get_image_data(self, image_path):\n",
        "    image_data = plt.imread(image_path)\n",
        "    return np.array(image_data)\n",
        "\n",
        "  def pred(self, image_data):\n",
        "    return self._model(image_data)\n",
        "\n",
        "  def pred_on_image_path(self, image_path):\n",
        "    image_data = np.array([self.get_image_data(image_path)])\n",
        "    return self.pred(image_data)\n",
        "  \n",
        "  def run_on_image(self, image_data):\n",
        "    pred = self.pred(np.array([image_data]))\n",
        "\n",
        "    image = Image.fromarray(image_data)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    Utils.add_blocks(pred,draw, image.size[1], image.size[0], self._font_path, self._precision)\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "  def run_on_image_with_path(self, image_path):\n",
        "    pred = self.pred_on_image_path(image_path)\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    Utils.add_blocks(pred,draw, image.size[1], image.size[0], self._font_path, self._precision)\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "  # Takes a folder path where the images should be in chronological order to \n",
        "  # detect and compile in a video\n",
        "\n",
        "  def run_on_folder(self, folder_path = Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v'), output_folder = Path().absolute().joinpath('Glie-44/Model/outputs'), framerate = 20):\n",
        "\n",
        "    video_frames = sorted(listdir(folder_path))\n",
        "    s = Image.open(Path(folder_path).joinpath(video_frames[0])).size\n",
        "\n",
        "    output_path = path.basename(folder_path) + '.avi'\n",
        "    fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "    writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "\n",
        "    sample = video_frames\n",
        "    total_frames = len(sample)\n",
        "    for i in range(total_frames):\n",
        "      #print(i + 1, '/', total_frames)\n",
        "      result = self.run_on_image_with_path(Path(folder_path).joinpath(sample[i]))\n",
        "      #print(sample[i], ':', result.shape)\n",
        "      writer.write(cv.cvtColor(result,cv.COLOR_RGB2BGR))\n",
        "\n",
        "    writer.release()\n",
        "\n",
        "  # Takes a video file to detect objects\n",
        "\n",
        "  def run_on_video(self, video_path, output_folder = Path().absolute().joinpath('Glie-44/Model/outputs'), framerate = 24):\n",
        "    cap = cv.VideoCapture(video_path)\n",
        "\n",
        "    output_path = Path(video_path).stem + '_output.avi'\n",
        "\n",
        "    if (cap.isOpened()):\n",
        "      s = (int(cap.get(3)),int(cap.get(4)))\n",
        "      fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "      writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      while ret == True:\n",
        "        result = self.run_on_image(frame)\n",
        "          \n",
        "        writer.write(result)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "  #  takes a video stream and outputs a video stream with the boxes around the\n",
        "  #  detected objects\n",
        "\n",
        "  def run_on_stream():\n",
        "    pass\n",
        "\n",
        "  def build_video_from_directory(self, folder_path = Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v'), output_folder = Path().absolute(), framerate = 24):\n",
        "    video_frames = sorted(listdir(folder_path))\n",
        "    s = Image.open(Path(folder_path).joinpath(video_frames[0])).size\n",
        "\n",
        "    output_path = 'Video_test.avi'\n",
        "    fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "    writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "\n",
        "    sample = video_frames\n",
        "    total_frames = len(sample)\n",
        "    for i in range(total_frames):\n",
        "      print(i + 1, '/', total_frames)\n",
        "      result = self.get_image_data(Path(folder_path).joinpath(sample[i]))#np.array(Image.open(Path(folder_path).joinpath(sample[i])))\n",
        "      #print(sample[i], ':', result.shape)\n",
        "      writer.write(cv.cvtColor(result,cv.COLOR_RGB2BGR))\n",
        "\n",
        "    writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dTQKkbavpZ"
      },
      "source": [
        "#model2 = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
        "model2 = hub.load(\"https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtmL4UJoavpa"
      },
      "source": [
        "glie = Glie_44()\n",
        "glie.set_model(model2)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#    TO TEST THE RUN_ON_IMAGE\n",
        "\n",
        "pred_image = glie.run_on_image_with_path('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg')\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(pred_image)\n",
        "\n",
        "pred = glie.pred_on_image_path('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg')\n",
        "pred\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "glie.run_on_video(video_path='Video_test.avi',output_folder=Path().absolute())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFCDWKdWavpa"
      },
      "source": [
        "#test = hub.KerasLayer(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\", trainable=True)\n",
        "test = hub.KerasLayer(\"https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1\", trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVCO4OB_b9fC",
        "outputId": "efb4dd22-a722-4ec6-9136-3945f3c8a436"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = np.array(Image.open('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg'))\n",
        "shape = x.shape\n",
        "x = np.array([])\n",
        "\n",
        "video_frames = sorted(listdir(folder_path))\n",
        "total_frames = len(video_frames)\n",
        "for i in range(total_frames):\n",
        "  x[i] = Image.open(Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v').joinpath(video_frames[i]))\n",
        "\n",
        "initial_layer = tf.keras.Input(shape=shape, name=\"initial_layer\",dtype=tf.uint8)\n",
        "layer = test(initial_layer)['detection_scores']# ou ['detection_boxes']\n",
        "print(layer)\n",
        "#layer = tf.keras.layers.Flatten()(initial_layer)\n",
        "layer = tf.keras.layers.Flatten()(layer)\n",
        "layer = tf.keras.layers.Dense(100, activation='relu',)(layer)\n",
        "last_layer = tf.keras.layers.Dense(10, activation='softmax')(layer)\n",
        "\n",
        "model = tf.keras.Model(initial_layer, last_layer)\n",
        "\n",
        "\n",
        "model.compile(optimizer=\"adam\", metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "model.fit(x=x, y=np.ones((1,10)), batch_size=1, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(1, 100), dtype=tf.float32, name=None), name='keras_layer/StatefulPartitionedCall:2', description=\"created by layer 'keras_layer'\")\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "initial_layer (InputLayer)   [(None, 756, 1344, 3)]    0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     {'detection_boxes': (1, 1 0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (1, 100)                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, 100)                  10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, 10)                   1010      \n",
            "=================================================================\n",
            "Total params: 11,110\n",
            "Trainable params: 11,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 10s 10s/step - loss: 23.3216 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1a89490f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd6FAKVtYPk_"
      },
      "source": [
        "class MultipleOutputLayer():\n",
        "  def __init__(self,layer = tf.keras.Input(shape=()), output_key = ''):\n",
        "    self._layer = layer\n",
        "    self._output_key = output_key\n",
        "\n",
        "  def set_output_key(self, key):\n",
        "    self._output_key = key\n",
        "\n",
        "  def get_output_key(self):\n",
        "    return self._output_key\n",
        "  \n",
        "  def set_layer(self, layer):\n",
        "    self._layer = layer\n",
        "\n",
        "  def get_layer(self):\n",
        "    return self._layer\n",
        "\n",
        "class Architecture():\n",
        "\n",
        "  def __init__(self, initial_layer = tf.keras.Input(shape=()), output_layer = tf.keras.layers.Dense(0)):\n",
        "    self._initial_layer = initial_layer\n",
        "    self._mid_layers = []\n",
        "    self._output_layer = output_layer\n",
        "\n",
        "  def set_initial_layer(self, layer):\n",
        "    self._initial_layer = layer\n",
        "\n",
        "  def get_initial_layer(self):\n",
        "    return self._initial_layer\n",
        "\n",
        "  def set_output_layer(self, layer):\n",
        "    self._output_layer = layer\n",
        "\n",
        "  def get_output_layer(self):\n",
        "    return self._output_layer\n",
        "\n",
        "  def get_mid_layers(self):\n",
        "    return self._mid_layers\n",
        "  \n",
        "  def __getitem__(self, layer_number):\n",
        "    return self._mid_layers[layer_number]\n",
        "  \n",
        "  def __setitem__(self, layer_number, data):\n",
        "    self._mid_layers[layer_number] = data\n",
        "  \n",
        "  def compile(self):\n",
        "    try :\n",
        "      mid_layer = self._initial_layer\n",
        "      for layer in self._mid_layers:\n",
        "        if type(layer) == MultipleOutputLayer:\n",
        "          mid_layer = layer.get_layer()(mid_layer)[layer.get_output_key()]\n",
        "        else:\n",
        "          mid_layer = layer(mid_layer)\n",
        "      return (self._initial_layer, self._output_layer(mid_layer))\n",
        "    except Exception as e:\n",
        "      print(f'Sorry. Something went wrong:\\n{e}')\n",
        "      pass\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, compiled_arc):\n",
        "    super().__init__(compiled_arc[0], compiled_arc[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecNkSvaP41QM",
        "outputId": "5cea128e-9981-4ca5-89ac-29ceff812be7"
      },
      "source": [
        "x = np.array(Image.open('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg'))\n",
        "shape = x.shape\n",
        "\n",
        "arc = Architecture()\n",
        "arc.set_initial_layer(tf.keras.Input(shape=shape, name=\"initial_layer\",dtype=tf.uint8))\n",
        "arc.get_mid_layers().append(MultipleOutputLayer(test, 'detection_scores'))\n",
        "arc.get_mid_layers().append(tf.keras.layers.Flatten())\n",
        "arc.set_output_layer(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "res = arc.compile()\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<KerasTensor: shape=(None, 756, 1344, 3) dtype=uint8 (created by layer 'initial_layer')>,\n",
              " <KerasTensor: shape=(1, 10) dtype=float32 (created by layer 'dense_11')>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDUCwkZR5OWD",
        "outputId": "f33a5260-b044-4d60-d163-cbdfbe31f4e8"
      },
      "source": [
        "model_test = Model(res)\n",
        "model_test.summary()\n",
        "model_test.compile(optimizer=\"adam\", metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "model_test.predict(np.array([x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "initial_layer (InputLayer)   [(None, 756, 1344, 3)]    0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     {'detection_boxes': (1, 1 0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (1, 100)                  0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (1, 10)                   1010      \n",
            "=================================================================\n",
            "Total params: 1,010\n",
            "Trainable params: 1,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09631571, 0.03675139, 0.13114575, 0.13017394, 0.09900831,\n",
              "        0.10070857, 0.17002572, 0.06272071, 0.08706453, 0.08608536]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWgZe8DQZS4"
      },
      "source": [
        "**New Model using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EcvQ0gWTmO"
      },
      "source": [
        "Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saxy2LvOmdbn",
        "outputId": "5daae9a5-7723-4a90-db4f-0a4651292c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 21862, done.\u001b[K\n",
            "remote: Counting objects: 100% (3250/3250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (712/712), done.\u001b[K\n",
            "remote: Total 21862 (delta 2561), reused 3091 (delta 2460), pack-reused 18612\u001b[K\n",
            "Receiving objects: 100% (21862/21862), 22.26 MiB | 19.63 MiB/s, done.\n",
            "Resolving deltas: 100% (16112/16112), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOtVOkjWSps"
      },
      "source": [
        "import torch\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "\n",
        "def main(train_percentage = .8, ):\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    torch.device('cpu')\n",
        "  \n",
        "  X = VisDroneDataset('PennFudanPed', to_tensor())\n",
        "\n",
        "  train_size = int(len(X) * train_percentage)\n",
        "  x_train, x_test = utils.data.random_split(X, [train_size, len(X) - train_size])\n",
        "\n",
        "  data_loader = utils.data.DataLoader(\n",
        "        x_train, batch_size=2, shuffle=True, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "  \n",
        "  model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = torch.optim.Adam()\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "  epochs = 10\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        lr_scheduler.step()\n",
        "        evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLmxWKCkRRan"
      },
      "source": [
        "class VisDroneDataset(object):\n",
        "    def __init__(self, root, preprocessing):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyCvAuqO12L1"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 11\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XhaZ_CZ6Z1"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKM3TFaZ6M8",
        "outputId": "4a5be8bc-ef77-4184-ac63-44e4c8b353c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = range(150)\n",
        "\n",
        "train_size = int(len(X) * .8)\n",
        "\n",
        "test = utils.data.random_split(X, [train_size, len(X) - train_size])\n",
        "len(test[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}