{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glie_44.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POE-DAMERON/Glie-44/blob/main/Model/Glie_44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPx23mf0avpR"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from os import path, listdir\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import io\n",
        "import random\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "  Downloads Data from the VisDrone dataset.\n",
        "  Input is which dataset to download:\n",
        "    - 0 is the training dataset\n",
        "    - 1 is the developper testing dataset\n",
        "    - 2 is the actual challenge testing dataset\n",
        "    - 3 is the val dataset\n",
        "'''\n",
        "\n",
        "def initialize_training(file = 0):\n",
        "  !git clone https://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@github.com/POE-DAMERON/Glie-44.git\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  \n",
        "  if file == 1:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-dev.zip\n",
        "  elif file == 2:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-challenge.zip\n",
        "  elif file == 3:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-val.zip\n",
        "  else:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-train.zip\n",
        "\n",
        "#initialize_training()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuaDM461NXLl",
        "outputId": "de5c2e61-82d3-4243-c035-612664264c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWgZe8DQZS4"
      },
      "source": [
        "**New Model using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EcvQ0gWTmO"
      },
      "source": [
        "Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saxy2LvOmdbn",
        "outputId": "2ee96d28-f4c5-4a1d-e276-79aaf4513781"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOtVOkjWSps"
      },
      "source": [
        "import torch\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import os\n",
        "import pandas as pd\n",
        "import transforms as T\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "\n",
        "def get_results(evaluator):\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "  result = str(evaluator.coco_eval)\n",
        "  test = evaluator.coco_eval.items()\n",
        "  for iou_type, coco_eval in test:\n",
        "    print(\"IoU metric: {}\".format(iou_type))\n",
        "    try:\n",
        "      print(coco_eval)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  sys.stdout = old_stdout\n",
        "\n",
        "  return buffer.getvalue()\n",
        "\n",
        "def add_to_record(arguments, output, is_saved = False, path_to_saved_model = ''):\n",
        "\n",
        "  with open('drive/MyDrive/training.csv', 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    #writer.writerow(['Saved', 'path_to_saved_model', 'number_of_epochs', 'batch_size', 'optimizer', 'learning_rate', 'weight_decay', 'momentum', 'lr_scheduler_step_size', 'lr_scheduler_gamma', 'output'])\n",
        "    writer.writerow([is_saved, path_to_saved_model, arguments['number_of_epochs'], arguments['batch_size'], arguments['optimizer'], arguments['lr'], arguments['weight_decay'], arguments['momentum'], arguments['lr_scheduler_step_size'], arguments['lr_scheduler_gamma'], output])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  saving the model will add it to the record\n",
        "\"\"\"\n",
        "\n",
        "def save_model(model, evaluator, arguments, path = ''):\n",
        "\n",
        "  \"\"\"\n",
        "      saves the model and adds to the csv records\n",
        "\n",
        "      2 methods :\n",
        "        - saving the whole model\n",
        "        - saving the weights and info\n",
        "  \"\"\"\n",
        "\n",
        "  if path == '':\n",
        "    path = 'drive/MyDrive/Models/model-' + str(int(time.time())) + '.pth'\n",
        "\n",
        "  torch.save(model,path) \n",
        "\n",
        "  #torch.save(model.state_dict(), path)\n",
        "  add_to_record(arguments = arguments, output = evaluator, is_saved = True, path_to_saved_model = path)\n",
        "  \n",
        "def load_model(path):\n",
        "\n",
        "  model = torch.load(path)\n",
        "\n",
        "  \"\"\"\n",
        "    model = Model()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "  \"\"\"\n",
        "\n",
        "  return model\n",
        "\n",
        "def save_checkpoints(dicti, path = 'drive/MyDrive/Checkpoints/checkpoint.txt'):\n",
        "  with open(path, 'w') as f:\n",
        "    f.write(json.dumps(dicti))\n",
        "\n",
        "def load_checkpoints(path = 'drive/MyDrive/Checkpoints/checkpoint.txt'):\n",
        "  dicti = {}\n",
        "  with open(path, 'r') as f:\n",
        "    dicti = json.loads(f.read())\n",
        "  return dicti\n",
        "\n",
        "def get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma):\n",
        "  \n",
        "  arguments = {}\n",
        "\n",
        "  arguments['train_percentage'] = train_percentage\n",
        "  if (optimizer == 'adam'):\n",
        "    arguments['optimizer'] = optimizer\n",
        "    arguments['momentum'] = ''\n",
        "  else:\n",
        "    arguments['optimizer'] = 'sgd'\n",
        "    arguments['momentum'] = momentum\n",
        "  arguments['lr'] = lr\n",
        "  arguments['weight_decay'] = weight_decay\n",
        "  arguments['lr_scheduler_step_size'] = step_size\n",
        "  arguments['lr_scheduler_gamma'] = gamma\n",
        "  arguments['number_of_epochs'] = epochs\n",
        "  arguments['batch_size'] = batch_size\n",
        "\n",
        "  return arguments\n",
        "\n",
        "def train_videos(path_to_saved_model = '', train_percentage = .8, test_size = -1,\n",
        "          train_size = -1, batch_size = 2, epochs = 10, optimizer='sgd',\n",
        "          lr = 0.005, momentum = 0.9, weight_decay= 0.0005, step_size = 3,\n",
        "          gamma = 0.1):\n",
        "  \n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  x = VisDroneDataset(Path().absolute().joinpath('VisDrone2019-MOT-train'), Utils.to_tensor())\n",
        "  testing_data = torch.utils.data.DataLoader(x[0])\n",
        "\n",
        "  for i in range(len(x)):\n",
        "\n",
        "    print(\"\\n-----------------------------\\nVideo {} / {}\\n\".format(i+1, len(x)))\n",
        "\n",
        "    X = x[i]\n",
        "\n",
        "    if train_size == -1:\n",
        "      train_sz = int(len(X) * train_percentage)\n",
        "    else:\n",
        "      train_sz = train_size\n",
        "      \n",
        "    x_train, x_test = torch.utils.data.random_split(X, [train_sz, len(X) - train_sz])\n",
        "\n",
        "    if test_size != -1:\n",
        "      x_test = Utils.slice(x_test, 0, test_size)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "          x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "          collate_fn=utils.collate_fn)\n",
        "    \n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "      x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "    testing_data = data_loader_test\n",
        "    \n",
        "    if str(path_to_saved_model) == '':\n",
        "      model = build_model()\n",
        "    else:\n",
        "      model = load_model(path_to_saved_model)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    if (optimizer == 'adam'):\n",
        "      optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "      optim = torch.optim.SGD(params, lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                    step_size=step_size,\n",
        "                                                    gamma=gamma)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "          train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "          lr_scheduler.step()\n",
        "          evaluate(model, testing_data, device=device)\n",
        "  \n",
        "  return model, evaluate(model, testing_data, device=device), arguments\n",
        "\n",
        "def train(path_to_saved_model = '', train_percentage = .8, test_percentage = -1,\n",
        "          test_size = -1, train_size = -1, batch_size = 2, epochs = 10, cur_epoch=0, optimizer='sgd',\n",
        "          lr = 0.005, momentum = 0.9, weight_decay= 0.0005, step_size = 3,\n",
        "          gamma = 0.1, checkpoints=-1, load_checkpoint=False):\n",
        "  \n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  X = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor())\n",
        "\n",
        "  if train_size == -1:\n",
        "    train_sz = int(len(X) * train_percentage)\n",
        "  else:\n",
        "    train_sz = train_size\n",
        "  \n",
        "  if test_percentage == -1:\n",
        "      test_sz = len(X) - train_sz\n",
        "  else:\n",
        "      test_sz = int(len(X) * min(1,max(0, test_percentage)))\n",
        "\n",
        "  x_train, x_test, x_overflow = torch.utils.data.random_split(X, [train_sz, test_sz , len(X) - train_sz - test_sz])\n",
        "\n",
        "  random.shuffle(X.imgs)\n",
        "\n",
        "  x_train = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor(), X.imgs[:train_sz])\n",
        "  x_test = AllVisDroneVideos(Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"sequences\"), Path().absolute().joinpath('VisDrone2019-MOT-train').joinpath(\"annotations\"), Utils.to_tensor(), X.imgs[train_sz:train_sz + test_sz])\n",
        "\n",
        "  if test_size != -1:\n",
        "    x_test = Utils.slice(x_test, 0, test_size)\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "      x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "  data_loader_test = torch.utils.data.DataLoader(\n",
        "      x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "      collate_fn=utils.collate_fn)\n",
        "    \n",
        "  if str(path_to_saved_model) == '':\n",
        "    model = build_model()\n",
        "  else:\n",
        "    model = load_model(path_to_saved_model)\n",
        "\n",
        "  if load_checkpoint:\n",
        "    checkpoint = load_checkpoints()\n",
        "    gamma = checkpoint['gamma']\n",
        "    cur_epoch = checkpoint['cur_epoch']\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "  if (optimizer == 'adam'):\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optim = torch.optim.SGD(params, lr=lr,\n",
        "                              momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                    step_size=step_size,\n",
        "                                                    gamma=gamma)\n",
        "\n",
        "  for epoch in range(cur_epoch + 1,epochs):\n",
        "    train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "    print('\\n---------\\nTRAIN ONE EPOCH FINISHED')\n",
        "    lr_scheduler.step()\n",
        "    #evaluate(model, data_loader_test, device=device)\n",
        "    results = get_results(evaluate(model, data_loader_test, device=device))\n",
        "    if checkpoints != -1 and epoch % checkpoints == 0:\n",
        "      save_model(model, results, arguments, path_to_saved_model)\n",
        "      save_checkpoints({'cur_epoch': epoch, 'gamma': gamma})\n",
        "      print('\\nModel Saved\\n')\n",
        "  \n",
        "  return model, results, arguments\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyCvAuqO12L1"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "def build_model(num_classes = 12):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  \"\"\"\n",
        "  backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "  backbone.out_channels = 1280\n",
        "\n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                  output_size=7,\n",
        "                                                  sampling_ratio=2)\n",
        "  return FasterRCNN(backbone,\n",
        "                    num_classes=num_classes,\n",
        "                    rpn_anchor_generator=anchor_generator,\n",
        "                    box_roi_pool=roi_pooler)\n",
        "  \"\"\"\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLmxWKCkRRan"
      },
      "source": [
        "class VisDroneVideo(object):\n",
        "    def __init__(self ,root, target_path, preprocessing = None, imgs = None):\n",
        "        self.root = str(root)\n",
        "        self.preprocessing = preprocessing\n",
        "        if (imgs != None):\n",
        "          self.imgs = imgs\n",
        "        else:\n",
        "          self.imgs = sorted(listdir(Path(root)), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.target = target_path\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(self.target)\n",
        "        image_targets = self.clean_targets(video_targets, idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img, target = self.preprocessing(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def clean_targets(self, targets, idx):\n",
        "\n",
        "        targets = self.targetsToDataframe(targets)\n",
        "        targets = targets[(targets.object_category != \"0\")]\n",
        "        targets = targets[(targets.frame_index == str(idx+1))]\n",
        "\n",
        "        targets.bbox_top = targets.bbox_top.astype('float32')\n",
        "        targets.bbox_height = targets.bbox_height.astype('float32')\n",
        "        targets.bbox_left = targets.bbox_left.astype('float32')\n",
        "        targets.bbox_width = targets.bbox_width.astype('float32')\n",
        "\n",
        "        targets[\"bottom\"] = targets.bbox_top + targets.bbox_height\n",
        "        targets[\"right\"] = targets.bbox_left + targets.bbox_width\n",
        "\n",
        "        return targets\n",
        "    \n",
        "    def targetsToDataframe(self, array):\n",
        "        columns = [\n",
        "          \"frame_index\",\n",
        "          \"target_id\",\n",
        "          \"bbox_left\",\n",
        "          \"bbox_top\",\n",
        "          \"bbox_width\",\n",
        "          \"bbox_height\",\n",
        "          \"score\",\n",
        "          \"object_category\",\n",
        "          \"truncation\",\n",
        "          \"oclusion\"\n",
        "        ]\n",
        "        return pd.DataFrame(data=array,columns=columns)\n",
        "\n",
        "class AllVisDroneVideos(VisDroneVideo):\n",
        "    def __init__(self,root, targets_path, preprocessing = None, imgs = None):\n",
        "      super().__init__(root, targets_path, preprocessing, imgs)\n",
        "      self.targets = sorted(listdir(Path(targets_path)), key=lambda x: x.lstrip(\"_\"))\n",
        "\n",
        "    def __len__(self):\n",
        "      return super().__len__()\n",
        "\n",
        "    def get_video_name(self, image_path):\n",
        "      return image_path.stem[:-8] + \".txt\", int(image_path.stem[-7:]) -1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_name, image_idx = self.get_video_name(img_path)\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(Path(self.target).joinpath(video_name))\n",
        "        image_targets = self.clean_targets(video_targets, image_idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img, target = self.preprocessing(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def get_video_from_idx(self,idx):\n",
        "\n",
        "      video_lengths = [269, 58, 118, 501, 181, 85, 217, 97, 361, 361,\n",
        " 516, 1255, 398, 412, 213, 256, 261, 307, 348, 225, 421, 680, 341, 768, 721,\n",
        " 677, 725, 616, 548, 116, 680, 872, 962, 547, 508, 1424, 500, 210, 346, 556, \n",
        " 414, 230, 185, 403, 632, 127, 426, 369, 196, 277, 196, 691, 421, 219, 462,296]\n",
        "      \n",
        "      for i in range(len(video_lengths)):\n",
        "        if idx<video_lengths[i]:\n",
        "          return i, idx\n",
        "        else:\n",
        "          idx -= video_lengths[i]\n",
        "      return len(video_lengths), idx\n",
        "\n",
        "class VisDroneDataset(object):\n",
        "    def __init__(self, root, preprocessing = None):\n",
        "        self.root = root\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        self.videos = sorted(listdir(Path(root).joinpath(\"sequences\")), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.targets = sorted(listdir(Path(root).joinpath(\"annotations\")), key=lambda x: x.lstrip(\"_\"))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      return VisDroneVideo(Path(self.root).joinpath(\"sequences\").joinpath(self.videos[idx]),Path(self.root).joinpath(\"annotations\").joinpath(self.targets[idx]), self.preprocessing)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    '''\n",
        "        Concatenates the images of all videos into a large VisDroneVideo class\n",
        "        Allows the model to train on a broader sample of data\n",
        "    \n",
        "\n",
        "    def all_images(self):\n",
        "        large_dataset = VisDroneVideo()\n",
        "\n",
        "        for i in self:\n",
        "          for j in i[0]:\n",
        "            print(j)\n",
        "\n",
        "    '''"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XhaZ_CZ6Z1"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKM3TFaZ6M8"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  @staticmethod\n",
        "  def to_tensor():\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "  @staticmethod\n",
        "  def read_txt_visdrone(path):\n",
        "      \"\"\"\n",
        "          Input: Path of the txt file with annotations as in the VisDrone dataset\\n\n",
        "          Output: A numpy array containing the information for bounding boxes\n",
        "      \"\"\"\n",
        "      lines = []\n",
        "      with open(path) as f:\n",
        "          lines = f.readlines()\n",
        "          f.close()\n",
        "      df = []\n",
        "      for x in lines:\n",
        "          splitLine = x.split(\",\")\n",
        "          splitLine[-1] = splitLine[-1].split(\"\\n\")[0]\n",
        "          df.append(splitLine)\n",
        "      return df\n",
        "      \n",
        "  @staticmethod\n",
        "  def bottom_right(left,top,width,height):\n",
        "      \"\"\"\n",
        "          Input: co-ordinates for the top left corner of a bounding box and its width and height\\n\n",
        "          Output: co-ordinates for the bottom right corner\n",
        "      \"\"\"\n",
        "      bottom = top + height\n",
        "      right = left + width\n",
        "      return right, bottom\n",
        "\n",
        "  @staticmethod\n",
        "  def slice(array, start, stop):\n",
        "    result = []\n",
        "    for i in range(stop-start):\n",
        "      result.append(array[start + i])\n",
        "    return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCt_fJY74uI",
        "outputId": "04499874-8c97-415d-eaef-5307f04c297d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#eval = train('drive/MyDrive/Models/model.pth', epochs = 100, train_percentage= .7, test_percentage = .1, load_checkpoint=False, checkpoints = 1, gamma=.0001, cur_epoch = 1)\n",
        "eval = train('drive/MyDrive/Models/model.pth', epochs = 100, train_percentage= .7, test_percentage = .1, load_checkpoint=True, checkpoints = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2]  [   0/8470]  eta: 4:01:12  lr: 0.005000  loss: 0.8815 (0.8815)  loss_classifier: 0.3669 (0.3669)  loss_box_reg: 0.4015 (0.4015)  loss_objectness: 0.0333 (0.0333)  loss_rpn_box_reg: 0.0798 (0.0798)  time: 1.7086  data: 0.9411  max mem: 4604\n",
            "Epoch: [2]  [  10/8470]  eta: 1:45:45  lr: 0.005000  loss: 0.7356 (0.6961)  loss_classifier: 0.2465 (0.2430)  loss_box_reg: 0.3457 (0.3398)  loss_objectness: 0.0338 (0.0332)  loss_rpn_box_reg: 0.0810 (0.0800)  time: 0.7500  data: 0.1071  max mem: 4636\n",
            "Epoch: [2]  [  20/8470]  eta: 1:38:50  lr: 0.005000  loss: 0.6941 (0.7021)  loss_classifier: 0.2432 (0.2372)  loss_box_reg: 0.3457 (0.3518)  loss_objectness: 0.0327 (0.0337)  loss_rpn_box_reg: 0.0777 (0.0794)  time: 0.6515  data: 0.0203  max mem: 5102\n",
            "Epoch: [2]  [  30/8470]  eta: 1:36:03  lr: 0.005000  loss: 0.6946 (0.7133)  loss_classifier: 0.2250 (0.2362)  loss_box_reg: 0.3500 (0.3511)  loss_objectness: 0.0328 (0.0383)  loss_rpn_box_reg: 0.0777 (0.0878)  time: 0.6459  data: 0.0156  max mem: 5102\n",
            "Epoch: [2]  [  40/8470]  eta: 1:34:46  lr: 0.005000  loss: 0.6549 (0.6951)  loss_classifier: 0.1931 (0.2278)  loss_box_reg: 0.3498 (0.3447)  loss_objectness: 0.0381 (0.0392)  loss_rpn_box_reg: 0.0823 (0.0833)  time: 0.6459  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [  50/8470]  eta: 1:34:05  lr: 0.005000  loss: 0.7024 (0.7074)  loss_classifier: 0.2405 (0.2349)  loss_box_reg: 0.3441 (0.3455)  loss_objectness: 0.0391 (0.0404)  loss_rpn_box_reg: 0.0823 (0.0866)  time: 0.6515  data: 0.0158  max mem: 5151\n",
            "Epoch: [2]  [  60/8470]  eta: 1:33:26  lr: 0.005000  loss: 0.7249 (0.7024)  loss_classifier: 0.2452 (0.2343)  loss_box_reg: 0.3444 (0.3432)  loss_objectness: 0.0400 (0.0394)  loss_rpn_box_reg: 0.0848 (0.0855)  time: 0.6506  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [  70/8470]  eta: 1:32:53  lr: 0.005000  loss: 0.6458 (0.6949)  loss_classifier: 0.2158 (0.2310)  loss_box_reg: 0.3444 (0.3416)  loss_objectness: 0.0361 (0.0392)  loss_rpn_box_reg: 0.0767 (0.0831)  time: 0.6458  data: 0.0161  max mem: 5151\n",
            "Epoch: [2]  [  80/8470]  eta: 1:32:16  lr: 0.005000  loss: 0.6458 (0.6945)  loss_classifier: 0.2158 (0.2306)  loss_box_reg: 0.3309 (0.3431)  loss_objectness: 0.0321 (0.0384)  loss_rpn_box_reg: 0.0720 (0.0825)  time: 0.6390  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [  90/8470]  eta: 1:31:50  lr: 0.005000  loss: 0.7161 (0.7038)  loss_classifier: 0.2352 (0.2326)  loss_box_reg: 0.3620 (0.3472)  loss_objectness: 0.0321 (0.0401)  loss_rpn_box_reg: 0.0854 (0.0839)  time: 0.6365  data: 0.0152  max mem: 5151\n",
            "Epoch: [2]  [ 100/8470]  eta: 1:31:21  lr: 0.005000  loss: 0.7161 (0.7005)  loss_classifier: 0.2466 (0.2333)  loss_box_reg: 0.3694 (0.3463)  loss_objectness: 0.0334 (0.0393)  loss_rpn_box_reg: 0.0773 (0.0816)  time: 0.6348  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [ 110/8470]  eta: 1:31:01  lr: 0.005000  loss: 0.6987 (0.7031)  loss_classifier: 0.2396 (0.2337)  loss_box_reg: 0.3605 (0.3473)  loss_objectness: 0.0268 (0.0395)  loss_rpn_box_reg: 0.0705 (0.0825)  time: 0.6337  data: 0.0161  max mem: 5151\n",
            "Epoch: [2]  [ 120/8470]  eta: 1:30:48  lr: 0.005000  loss: 0.7157 (0.7067)  loss_classifier: 0.2362 (0.2346)  loss_box_reg: 0.3685 (0.3494)  loss_objectness: 0.0282 (0.0393)  loss_rpn_box_reg: 0.0759 (0.0834)  time: 0.6408  data: 0.0161  max mem: 5151\n",
            "Epoch: [2]  [ 130/8470]  eta: 1:30:33  lr: 0.005000  loss: 0.7477 (0.7127)  loss_classifier: 0.2417 (0.2369)  loss_box_reg: 0.3750 (0.3508)  loss_objectness: 0.0335 (0.0398)  loss_rpn_box_reg: 0.0945 (0.0852)  time: 0.6415  data: 0.0159  max mem: 5151\n",
            "Epoch: [2]  [ 140/8470]  eta: 1:30:21  lr: 0.005000  loss: 0.7491 (0.7148)  loss_classifier: 0.2485 (0.2372)  loss_box_reg: 0.3717 (0.3524)  loss_objectness: 0.0379 (0.0402)  loss_rpn_box_reg: 0.0945 (0.0850)  time: 0.6405  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 150/8470]  eta: 1:30:08  lr: 0.005000  loss: 0.7491 (0.7161)  loss_classifier: 0.2643 (0.2392)  loss_box_reg: 0.3717 (0.3532)  loss_objectness: 0.0361 (0.0397)  loss_rpn_box_reg: 0.0734 (0.0841)  time: 0.6405  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 160/8470]  eta: 1:29:54  lr: 0.005000  loss: 0.7300 (0.7156)  loss_classifier: 0.2482 (0.2387)  loss_box_reg: 0.3678 (0.3537)  loss_objectness: 0.0293 (0.0394)  loss_rpn_box_reg: 0.0700 (0.0838)  time: 0.6369  data: 0.0139  max mem: 5151\n",
            "Epoch: [2]  [ 170/8470]  eta: 1:29:42  lr: 0.005000  loss: 0.7016 (0.7168)  loss_classifier: 0.2327 (0.2397)  loss_box_reg: 0.3561 (0.3543)  loss_objectness: 0.0331 (0.0394)  loss_rpn_box_reg: 0.0676 (0.0834)  time: 0.6371  data: 0.0135  max mem: 5151\n",
            "Epoch: [2]  [ 180/8470]  eta: 1:29:30  lr: 0.005000  loss: 0.6983 (0.7125)  loss_classifier: 0.2222 (0.2375)  loss_box_reg: 0.3541 (0.3536)  loss_objectness: 0.0367 (0.0388)  loss_rpn_box_reg: 0.0672 (0.0826)  time: 0.6371  data: 0.0149  max mem: 5151\n",
            "Epoch: [2]  [ 190/8470]  eta: 1:29:19  lr: 0.005000  loss: 0.7144 (0.7150)  loss_classifier: 0.2222 (0.2384)  loss_box_reg: 0.3602 (0.3547)  loss_objectness: 0.0287 (0.0391)  loss_rpn_box_reg: 0.0793 (0.0828)  time: 0.6363  data: 0.0138  max mem: 5151\n",
            "Epoch: [2]  [ 200/8470]  eta: 1:29:08  lr: 0.005000  loss: 0.7280 (0.7138)  loss_classifier: 0.2463 (0.2376)  loss_box_reg: 0.3666 (0.3544)  loss_objectness: 0.0367 (0.0391)  loss_rpn_box_reg: 0.0754 (0.0826)  time: 0.6373  data: 0.0142  max mem: 5151\n",
            "Epoch: [2]  [ 210/8470]  eta: 1:29:00  lr: 0.005000  loss: 0.6993 (0.7132)  loss_classifier: 0.2130 (0.2362)  loss_box_reg: 0.3585 (0.3546)  loss_objectness: 0.0367 (0.0391)  loss_rpn_box_reg: 0.0740 (0.0833)  time: 0.6397  data: 0.0155  max mem: 5151\n",
            "Epoch: [2]  [ 220/8470]  eta: 1:28:49  lr: 0.005000  loss: 0.7384 (0.7147)  loss_classifier: 0.2327 (0.2367)  loss_box_reg: 0.3599 (0.3552)  loss_objectness: 0.0417 (0.0393)  loss_rpn_box_reg: 0.0890 (0.0835)  time: 0.6392  data: 0.0149  max mem: 5151\n",
            "Epoch: [2]  [ 230/8470]  eta: 1:28:40  lr: 0.005000  loss: 0.7384 (0.7130)  loss_classifier: 0.2327 (0.2353)  loss_box_reg: 0.3459 (0.3541)  loss_objectness: 0.0417 (0.0395)  loss_rpn_box_reg: 0.0947 (0.0841)  time: 0.6372  data: 0.0150  max mem: 5151\n",
            "Epoch: [2]  [ 240/8470]  eta: 1:28:31  lr: 0.005000  loss: 0.6958 (0.7135)  loss_classifier: 0.2232 (0.2355)  loss_box_reg: 0.3277 (0.3535)  loss_objectness: 0.0425 (0.0399)  loss_rpn_box_reg: 0.0947 (0.0846)  time: 0.6377  data: 0.0148  max mem: 5151\n",
            "Epoch: [2]  [ 250/8470]  eta: 1:28:21  lr: 0.005000  loss: 0.6876 (0.7112)  loss_classifier: 0.2194 (0.2343)  loss_box_reg: 0.3277 (0.3525)  loss_objectness: 0.0396 (0.0400)  loss_rpn_box_reg: 0.0832 (0.0844)  time: 0.6356  data: 0.0145  max mem: 5151\n",
            "Epoch: [2]  [ 260/8470]  eta: 1:28:11  lr: 0.005000  loss: 0.6432 (0.7093)  loss_classifier: 0.2043 (0.2339)  loss_box_reg: 0.3126 (0.3519)  loss_objectness: 0.0324 (0.0396)  loss_rpn_box_reg: 0.0741 (0.0839)  time: 0.6340  data: 0.0152  max mem: 5151\n",
            "Epoch: [2]  [ 270/8470]  eta: 1:28:03  lr: 0.005000  loss: 0.6711 (0.7124)  loss_classifier: 0.2257 (0.2348)  loss_box_reg: 0.3409 (0.3528)  loss_objectness: 0.0402 (0.0401)  loss_rpn_box_reg: 0.0790 (0.0847)  time: 0.6375  data: 0.0155  max mem: 5151\n",
            "Epoch: [2]  [ 280/8470]  eta: 1:27:56  lr: 0.005000  loss: 0.7433 (0.7105)  loss_classifier: 0.2409 (0.2345)  loss_box_reg: 0.3604 (0.3521)  loss_objectness: 0.0402 (0.0399)  loss_rpn_box_reg: 0.0763 (0.0840)  time: 0.6414  data: 0.0149  max mem: 5151\n",
            "Epoch: [2]  [ 290/8470]  eta: 1:27:49  lr: 0.005000  loss: 0.6957 (0.7115)  loss_classifier: 0.2318 (0.2346)  loss_box_reg: 0.3591 (0.3527)  loss_objectness: 0.0354 (0.0398)  loss_rpn_box_reg: 0.0763 (0.0844)  time: 0.6421  data: 0.0153  max mem: 5151\n",
            "Epoch: [2]  [ 300/8470]  eta: 1:27:38  lr: 0.005000  loss: 0.6726 (0.7108)  loss_classifier: 0.2136 (0.2341)  loss_box_reg: 0.3689 (0.3533)  loss_objectness: 0.0348 (0.0394)  loss_rpn_box_reg: 0.0792 (0.0839)  time: 0.6352  data: 0.0146  max mem: 5151\n",
            "Epoch: [2]  [ 310/8470]  eta: 1:27:32  lr: 0.005000  loss: 0.7071 (0.7116)  loss_classifier: 0.2184 (0.2345)  loss_box_reg: 0.3740 (0.3536)  loss_objectness: 0.0307 (0.0394)  loss_rpn_box_reg: 0.0686 (0.0840)  time: 0.6358  data: 0.0140  max mem: 5151\n",
            "Epoch: [2]  [ 320/8470]  eta: 1:27:25  lr: 0.005000  loss: 0.7306 (0.7133)  loss_classifier: 0.2394 (0.2351)  loss_box_reg: 0.3849 (0.3545)  loss_objectness: 0.0326 (0.0394)  loss_rpn_box_reg: 0.0890 (0.0843)  time: 0.6438  data: 0.0165  max mem: 5151\n",
            "Epoch: [2]  [ 330/8470]  eta: 1:27:21  lr: 0.005000  loss: 0.6884 (0.7096)  loss_classifier: 0.2126 (0.2335)  loss_box_reg: 0.3250 (0.3530)  loss_objectness: 0.0281 (0.0390)  loss_rpn_box_reg: 0.0890 (0.0841)  time: 0.6473  data: 0.0175  max mem: 5151\n",
            "Epoch: [2]  [ 340/8470]  eta: 1:27:14  lr: 0.005000  loss: 0.6973 (0.7120)  loss_classifier: 0.2137 (0.2350)  loss_box_reg: 0.3589 (0.3539)  loss_objectness: 0.0281 (0.0390)  loss_rpn_box_reg: 0.0829 (0.0840)  time: 0.6466  data: 0.0157  max mem: 5151\n",
            "Epoch: [2]  [ 350/8470]  eta: 1:27:05  lr: 0.005000  loss: 0.6873 (0.7101)  loss_classifier: 0.2223 (0.2348)  loss_box_reg: 0.3589 (0.3533)  loss_objectness: 0.0263 (0.0385)  loss_rpn_box_reg: 0.0746 (0.0834)  time: 0.6378  data: 0.0138  max mem: 5151\n",
            "Epoch: [2]  [ 360/8470]  eta: 1:26:58  lr: 0.005000  loss: 0.6822 (0.7114)  loss_classifier: 0.2304 (0.2353)  loss_box_reg: 0.3558 (0.3542)  loss_objectness: 0.0234 (0.0383)  loss_rpn_box_reg: 0.0669 (0.0835)  time: 0.6358  data: 0.0143  max mem: 5151\n",
            "Epoch: [2]  [ 370/8470]  eta: 1:26:49  lr: 0.005000  loss: 0.7122 (0.7109)  loss_classifier: 0.2312 (0.2352)  loss_box_reg: 0.3578 (0.3540)  loss_objectness: 0.0297 (0.0384)  loss_rpn_box_reg: 0.0838 (0.0833)  time: 0.6368  data: 0.0147  max mem: 5151\n",
            "Epoch: [2]  [ 380/8470]  eta: 1:26:44  lr: 0.005000  loss: 0.7122 (0.7114)  loss_classifier: 0.2290 (0.2353)  loss_box_reg: 0.3664 (0.3544)  loss_objectness: 0.0297 (0.0383)  loss_rpn_box_reg: 0.0792 (0.0834)  time: 0.6410  data: 0.0149  max mem: 5151\n",
            "Epoch: [2]  [ 390/8470]  eta: 1:26:39  lr: 0.005000  loss: 0.7251 (0.7110)  loss_classifier: 0.2404 (0.2354)  loss_box_reg: 0.3616 (0.3542)  loss_objectness: 0.0298 (0.0382)  loss_rpn_box_reg: 0.0692 (0.0832)  time: 0.6506  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 400/8470]  eta: 1:26:34  lr: 0.005000  loss: 0.6792 (0.7105)  loss_classifier: 0.2292 (0.2352)  loss_box_reg: 0.3309 (0.3539)  loss_objectness: 0.0307 (0.0382)  loss_rpn_box_reg: 0.0663 (0.0832)  time: 0.6513  data: 0.0176  max mem: 5151\n",
            "Epoch: [2]  [ 410/8470]  eta: 1:26:28  lr: 0.005000  loss: 0.6695 (0.7092)  loss_classifier: 0.2193 (0.2346)  loss_box_reg: 0.3365 (0.3537)  loss_objectness: 0.0245 (0.0380)  loss_rpn_box_reg: 0.0653 (0.0828)  time: 0.6464  data: 0.0172  max mem: 5151\n",
            "Epoch: [2]  [ 420/8470]  eta: 1:26:22  lr: 0.005000  loss: 0.6670 (0.7090)  loss_classifier: 0.2193 (0.2346)  loss_box_reg: 0.3247 (0.3533)  loss_objectness: 0.0347 (0.0380)  loss_rpn_box_reg: 0.0770 (0.0831)  time: 0.6451  data: 0.0167  max mem: 5151\n",
            "Epoch: [2]  [ 430/8470]  eta: 1:26:14  lr: 0.005000  loss: 0.7496 (0.7103)  loss_classifier: 0.2395 (0.2352)  loss_box_reg: 0.3761 (0.3539)  loss_objectness: 0.0347 (0.0380)  loss_rpn_box_reg: 0.0852 (0.0831)  time: 0.6407  data: 0.0164  max mem: 5151\n",
            "Epoch: [2]  [ 440/8470]  eta: 1:26:07  lr: 0.005000  loss: 0.7562 (0.7119)  loss_classifier: 0.2395 (0.2360)  loss_box_reg: 0.3866 (0.3549)  loss_objectness: 0.0340 (0.0380)  loss_rpn_box_reg: 0.0737 (0.0830)  time: 0.6396  data: 0.0156  max mem: 5151\n",
            "Epoch: [2]  [ 450/8470]  eta: 1:26:01  lr: 0.005000  loss: 0.7519 (0.7129)  loss_classifier: 0.2213 (0.2361)  loss_box_reg: 0.3890 (0.3554)  loss_objectness: 0.0340 (0.0382)  loss_rpn_box_reg: 0.0900 (0.0831)  time: 0.6441  data: 0.0170  max mem: 5151\n",
            "Epoch: [2]  [ 460/8470]  eta: 1:25:54  lr: 0.005000  loss: 0.7280 (0.7119)  loss_classifier: 0.2078 (0.2356)  loss_box_reg: 0.3856 (0.3552)  loss_objectness: 0.0260 (0.0380)  loss_rpn_box_reg: 0.0900 (0.0831)  time: 0.6427  data: 0.0180  max mem: 5151\n",
            "Epoch: [2]  [ 470/8470]  eta: 1:25:49  lr: 0.005000  loss: 0.6719 (0.7110)  loss_classifier: 0.2191 (0.2354)  loss_box_reg: 0.3497 (0.3550)  loss_objectness: 0.0292 (0.0378)  loss_rpn_box_reg: 0.0755 (0.0827)  time: 0.6465  data: 0.0204  max mem: 5151\n",
            "Epoch: [2]  [ 480/8470]  eta: 1:25:42  lr: 0.005000  loss: 0.6525 (0.7104)  loss_classifier: 0.2209 (0.2350)  loss_box_reg: 0.3287 (0.3548)  loss_objectness: 0.0308 (0.0378)  loss_rpn_box_reg: 0.0751 (0.0829)  time: 0.6455  data: 0.0187  max mem: 5151\n",
            "Epoch: [2]  [ 490/8470]  eta: 1:25:34  lr: 0.005000  loss: 0.6751 (0.7105)  loss_classifier: 0.2209 (0.2351)  loss_box_reg: 0.3410 (0.3549)  loss_objectness: 0.0308 (0.0378)  loss_rpn_box_reg: 0.0728 (0.0827)  time: 0.6370  data: 0.0148  max mem: 5151\n",
            "Epoch: [2]  [ 500/8470]  eta: 1:25:29  lr: 0.005000  loss: 0.6514 (0.7094)  loss_classifier: 0.2161 (0.2349)  loss_box_reg: 0.3398 (0.3544)  loss_objectness: 0.0320 (0.0377)  loss_rpn_box_reg: 0.0699 (0.0825)  time: 0.6425  data: 0.0167  max mem: 5151\n",
            "Epoch: [2]  [ 510/8470]  eta: 1:25:22  lr: 0.005000  loss: 0.6797 (0.7098)  loss_classifier: 0.2334 (0.2354)  loss_box_reg: 0.3410 (0.3543)  loss_objectness: 0.0312 (0.0377)  loss_rpn_box_reg: 0.0753 (0.0824)  time: 0.6448  data: 0.0179  max mem: 5151\n",
            "Epoch: [2]  [ 520/8470]  eta: 1:25:14  lr: 0.005000  loss: 0.6902 (0.7090)  loss_classifier: 0.2327 (0.2353)  loss_box_reg: 0.3486 (0.3542)  loss_objectness: 0.0280 (0.0375)  loss_rpn_box_reg: 0.0661 (0.0821)  time: 0.6390  data: 0.0168  max mem: 5151\n",
            "Epoch: [2]  [ 530/8470]  eta: 1:25:08  lr: 0.005000  loss: 0.7053 (0.7105)  loss_classifier: 0.2314 (0.2353)  loss_box_reg: 0.3673 (0.3548)  loss_objectness: 0.0263 (0.0375)  loss_rpn_box_reg: 0.0877 (0.0828)  time: 0.6401  data: 0.0163  max mem: 5151\n",
            "Epoch: [2]  [ 540/8470]  eta: 1:25:01  lr: 0.005000  loss: 0.7600 (0.7107)  loss_classifier: 0.2402 (0.2354)  loss_box_reg: 0.3778 (0.3548)  loss_objectness: 0.0271 (0.0375)  loss_rpn_box_reg: 0.1011 (0.0830)  time: 0.6413  data: 0.0168  max mem: 5151\n",
            "Epoch: [2]  [ 550/8470]  eta: 1:24:54  lr: 0.005000  loss: 0.7435 (0.7113)  loss_classifier: 0.2322 (0.2354)  loss_box_reg: 0.3658 (0.3552)  loss_objectness: 0.0315 (0.0375)  loss_rpn_box_reg: 0.0939 (0.0833)  time: 0.6390  data: 0.0160  max mem: 5151\n",
            "Epoch: [2]  [ 560/8470]  eta: 1:24:46  lr: 0.005000  loss: 0.7156 (0.7117)  loss_classifier: 0.2370 (0.2355)  loss_box_reg: 0.3658 (0.3553)  loss_objectness: 0.0336 (0.0374)  loss_rpn_box_reg: 0.0921 (0.0834)  time: 0.6359  data: 0.0151  max mem: 5151\n",
            "Epoch: [2]  [ 570/8470]  eta: 1:24:39  lr: 0.005000  loss: 0.7156 (0.7109)  loss_classifier: 0.2264 (0.2351)  loss_box_reg: 0.3552 (0.3551)  loss_objectness: 0.0348 (0.0374)  loss_rpn_box_reg: 0.0921 (0.0833)  time: 0.6382  data: 0.0166  max mem: 5151\n",
            "Epoch: [2]  [ 580/8470]  eta: 1:24:34  lr: 0.005000  loss: 0.6659 (0.7107)  loss_classifier: 0.2098 (0.2348)  loss_box_reg: 0.3525 (0.3551)  loss_objectness: 0.0253 (0.0372)  loss_rpn_box_reg: 0.0896 (0.0836)  time: 0.6456  data: 0.0175  max mem: 5151\n",
            "Epoch: [2]  [ 590/8470]  eta: 1:24:27  lr: 0.005000  loss: 0.6659 (0.7099)  loss_classifier: 0.2098 (0.2344)  loss_box_reg: 0.3531 (0.3549)  loss_objectness: 0.0241 (0.0371)  loss_rpn_box_reg: 0.0751 (0.0835)  time: 0.6437  data: 0.0159  max mem: 5151\n",
            "Epoch: [2]  [ 600/8470]  eta: 1:24:20  lr: 0.005000  loss: 0.6785 (0.7095)  loss_classifier: 0.2226 (0.2342)  loss_box_reg: 0.3657 (0.3551)  loss_objectness: 0.0223 (0.0369)  loss_rpn_box_reg: 0.0718 (0.0833)  time: 0.6400  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [ 610/8470]  eta: 1:24:13  lr: 0.005000  loss: 0.7147 (0.7095)  loss_classifier: 0.2387 (0.2344)  loss_box_reg: 0.3657 (0.3551)  loss_objectness: 0.0270 (0.0369)  loss_rpn_box_reg: 0.0715 (0.0832)  time: 0.6399  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [ 620/8470]  eta: 1:24:06  lr: 0.005000  loss: 0.7242 (0.7091)  loss_classifier: 0.2435 (0.2341)  loss_box_reg: 0.3657 (0.3552)  loss_objectness: 0.0279 (0.0368)  loss_rpn_box_reg: 0.0757 (0.0830)  time: 0.6379  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 630/8470]  eta: 1:24:00  lr: 0.005000  loss: 0.6425 (0.7073)  loss_classifier: 0.2062 (0.2335)  loss_box_reg: 0.3336 (0.3544)  loss_objectness: 0.0260 (0.0366)  loss_rpn_box_reg: 0.0708 (0.0828)  time: 0.6411  data: 0.0175  max mem: 5151\n",
            "Epoch: [2]  [ 640/8470]  eta: 1:23:53  lr: 0.005000  loss: 0.6432 (0.7077)  loss_classifier: 0.2141 (0.2336)  loss_box_reg: 0.3336 (0.3546)  loss_objectness: 0.0244 (0.0367)  loss_rpn_box_reg: 0.0698 (0.0828)  time: 0.6407  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [ 650/8470]  eta: 1:23:46  lr: 0.005000  loss: 0.7176 (0.7071)  loss_classifier: 0.2339 (0.2333)  loss_box_reg: 0.3450 (0.3543)  loss_objectness: 0.0232 (0.0368)  loss_rpn_box_reg: 0.0794 (0.0827)  time: 0.6391  data: 0.0151  max mem: 5151\n",
            "Epoch: [2]  [ 660/8470]  eta: 1:23:39  lr: 0.005000  loss: 0.7136 (0.7070)  loss_classifier: 0.2127 (0.2332)  loss_box_reg: 0.3401 (0.3543)  loss_objectness: 0.0350 (0.0368)  loss_rpn_box_reg: 0.0794 (0.0826)  time: 0.6396  data: 0.0161  max mem: 5151\n",
            "Epoch: [2]  [ 670/8470]  eta: 1:23:33  lr: 0.005000  loss: 0.7136 (0.7066)  loss_classifier: 0.2127 (0.2331)  loss_box_reg: 0.3594 (0.3543)  loss_objectness: 0.0447 (0.0368)  loss_rpn_box_reg: 0.0779 (0.0825)  time: 0.6425  data: 0.0161  max mem: 5151\n",
            "Epoch: [2]  [ 680/8470]  eta: 1:23:26  lr: 0.005000  loss: 0.6816 (0.7057)  loss_classifier: 0.2119 (0.2326)  loss_box_reg: 0.3532 (0.3540)  loss_objectness: 0.0260 (0.0368)  loss_rpn_box_reg: 0.0755 (0.0823)  time: 0.6434  data: 0.0165  max mem: 5151\n",
            "Epoch: [2]  [ 690/8470]  eta: 1:23:18  lr: 0.005000  loss: 0.6549 (0.7054)  loss_classifier: 0.2024 (0.2324)  loss_box_reg: 0.3532 (0.3542)  loss_objectness: 0.0194 (0.0366)  loss_rpn_box_reg: 0.0665 (0.0821)  time: 0.6350  data: 0.0155  max mem: 5151\n",
            "Epoch: [2]  [ 700/8470]  eta: 1:23:11  lr: 0.005000  loss: 0.6684 (0.7051)  loss_classifier: 0.2146 (0.2322)  loss_box_reg: 0.3615 (0.3541)  loss_objectness: 0.0236 (0.0366)  loss_rpn_box_reg: 0.0663 (0.0821)  time: 0.6324  data: 0.0144  max mem: 5151\n",
            "Epoch: [2]  [ 710/8470]  eta: 1:23:05  lr: 0.005000  loss: 0.6912 (0.7055)  loss_classifier: 0.2232 (0.2323)  loss_box_reg: 0.3539 (0.3544)  loss_objectness: 0.0287 (0.0367)  loss_rpn_box_reg: 0.0777 (0.0821)  time: 0.6380  data: 0.0150  max mem: 5151\n",
            "Epoch: [2]  [ 720/8470]  eta: 1:22:59  lr: 0.005000  loss: 0.6912 (0.7053)  loss_classifier: 0.2214 (0.2322)  loss_box_reg: 0.3467 (0.3542)  loss_objectness: 0.0350 (0.0368)  loss_rpn_box_reg: 0.0846 (0.0822)  time: 0.6456  data: 0.0173  max mem: 5151\n",
            "Epoch: [2]  [ 730/8470]  eta: 1:22:53  lr: 0.005000  loss: 0.6882 (0.7061)  loss_classifier: 0.2214 (0.2325)  loss_box_reg: 0.3467 (0.3544)  loss_objectness: 0.0417 (0.0369)  loss_rpn_box_reg: 0.0850 (0.0824)  time: 0.6467  data: 0.0167  max mem: 5151\n",
            "Epoch: [2]  [ 740/8470]  eta: 1:22:45  lr: 0.005000  loss: 0.7770 (0.7067)  loss_classifier: 0.2535 (0.2328)  loss_box_reg: 0.3700 (0.3549)  loss_objectness: 0.0417 (0.0369)  loss_rpn_box_reg: 0.0769 (0.0822)  time: 0.6358  data: 0.0141  max mem: 5151\n",
            "Epoch: [2]  [ 750/8470]  eta: 1:22:39  lr: 0.005000  loss: 0.7518 (0.7068)  loss_classifier: 0.2538 (0.2329)  loss_box_reg: 0.3716 (0.3548)  loss_objectness: 0.0381 (0.0369)  loss_rpn_box_reg: 0.0699 (0.0821)  time: 0.6386  data: 0.0151  max mem: 5151\n",
            "Epoch: [2]  [ 760/8470]  eta: 1:22:33  lr: 0.005000  loss: 0.7308 (0.7062)  loss_classifier: 0.2389 (0.2327)  loss_box_reg: 0.3319 (0.3544)  loss_objectness: 0.0348 (0.0370)  loss_rpn_box_reg: 0.0736 (0.0822)  time: 0.6486  data: 0.0165  max mem: 5151\n",
            "Epoch: [2]  [ 770/8470]  eta: 1:22:26  lr: 0.005000  loss: 0.7116 (0.7059)  loss_classifier: 0.2139 (0.2326)  loss_box_reg: 0.3225 (0.3542)  loss_objectness: 0.0278 (0.0369)  loss_rpn_box_reg: 0.0866 (0.0823)  time: 0.6416  data: 0.0149  max mem: 5151\n",
            "Epoch: [2]  [ 780/8470]  eta: 1:22:20  lr: 0.005000  loss: 0.6854 (0.7056)  loss_classifier: 0.2223 (0.2325)  loss_box_reg: 0.3427 (0.3542)  loss_objectness: 0.0258 (0.0368)  loss_rpn_box_reg: 0.0740 (0.0821)  time: 0.6393  data: 0.0155  max mem: 5151\n",
            "Epoch: [2]  [ 790/8470]  eta: 1:22:14  lr: 0.005000  loss: 0.6778 (0.7054)  loss_classifier: 0.2248 (0.2324)  loss_box_reg: 0.3505 (0.3542)  loss_objectness: 0.0227 (0.0368)  loss_rpn_box_reg: 0.0709 (0.0820)  time: 0.6465  data: 0.0177  max mem: 5151\n",
            "Epoch: [2]  [ 800/8470]  eta: 1:22:08  lr: 0.005000  loss: 0.6828 (0.7057)  loss_classifier: 0.2195 (0.2322)  loss_box_reg: 0.3510 (0.3543)  loss_objectness: 0.0294 (0.0369)  loss_rpn_box_reg: 0.0784 (0.0824)  time: 0.6506  data: 0.0181  max mem: 5151\n",
            "Epoch: [2]  [ 810/8470]  eta: 1:22:01  lr: 0.005000  loss: 0.6828 (0.7055)  loss_classifier: 0.2177 (0.2321)  loss_box_reg: 0.3510 (0.3544)  loss_objectness: 0.0285 (0.0367)  loss_rpn_box_reg: 0.0771 (0.0822)  time: 0.6440  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 820/8470]  eta: 1:21:54  lr: 0.005000  loss: 0.6690 (0.7057)  loss_classifier: 0.2176 (0.2321)  loss_box_reg: 0.3423 (0.3546)  loss_objectness: 0.0246 (0.0367)  loss_rpn_box_reg: 0.0680 (0.0822)  time: 0.6341  data: 0.0139  max mem: 5151\n",
            "Epoch: [2]  [ 830/8470]  eta: 1:21:47  lr: 0.005000  loss: 0.6690 (0.7048)  loss_classifier: 0.2051 (0.2317)  loss_box_reg: 0.3311 (0.3543)  loss_objectness: 0.0232 (0.0365)  loss_rpn_box_reg: 0.0650 (0.0822)  time: 0.6338  data: 0.0141  max mem: 5151\n",
            "Epoch: [2]  [ 840/8470]  eta: 1:21:41  lr: 0.005000  loss: 0.7225 (0.7057)  loss_classifier: 0.2372 (0.2323)  loss_box_reg: 0.3671 (0.3547)  loss_objectness: 0.0242 (0.0365)  loss_rpn_box_reg: 0.0824 (0.0822)  time: 0.6397  data: 0.0153  max mem: 5151\n",
            "Epoch: [2]  [ 850/8470]  eta: 1:21:35  lr: 0.005000  loss: 0.7634 (0.7064)  loss_classifier: 0.2464 (0.2326)  loss_box_reg: 0.3825 (0.3550)  loss_objectness: 0.0343 (0.0366)  loss_rpn_box_reg: 0.0852 (0.0822)  time: 0.6450  data: 0.0157  max mem: 5151\n",
            "Epoch: [2]  [ 860/8470]  eta: 1:21:28  lr: 0.005000  loss: 0.7357 (0.7063)  loss_classifier: 0.2329 (0.2324)  loss_box_reg: 0.3797 (0.3553)  loss_objectness: 0.0336 (0.0365)  loss_rpn_box_reg: 0.0755 (0.0821)  time: 0.6427  data: 0.0162  max mem: 5151\n",
            "Epoch: [2]  [ 870/8470]  eta: 1:21:21  lr: 0.005000  loss: 0.7303 (0.7066)  loss_classifier: 0.2130 (0.2325)  loss_box_reg: 0.3654 (0.3554)  loss_objectness: 0.0266 (0.0365)  loss_rpn_box_reg: 0.0773 (0.0822)  time: 0.6375  data: 0.0165  max mem: 5151\n",
            "Epoch: [2]  [ 880/8470]  eta: 1:21:14  lr: 0.005000  loss: 0.7420 (0.7075)  loss_classifier: 0.2284 (0.2327)  loss_box_reg: 0.3707 (0.3557)  loss_objectness: 0.0297 (0.0367)  loss_rpn_box_reg: 0.0979 (0.0823)  time: 0.6378  data: 0.0148  max mem: 5151\n",
            "Epoch: [2]  [ 890/8470]  eta: 1:21:07  lr: 0.005000  loss: 0.7013 (0.7071)  loss_classifier: 0.2191 (0.2325)  loss_box_reg: 0.3688 (0.3557)  loss_objectness: 0.0427 (0.0367)  loss_rpn_box_reg: 0.0941 (0.0821)  time: 0.6392  data: 0.0144  max mem: 5151\n",
            "Epoch: [2]  [ 900/8470]  eta: 1:21:01  lr: 0.005000  loss: 0.6393 (0.7067)  loss_classifier: 0.2022 (0.2323)  loss_box_reg: 0.3372 (0.3555)  loss_objectness: 0.0353 (0.0368)  loss_rpn_box_reg: 0.0523 (0.0821)  time: 0.6407  data: 0.0160  max mem: 5151\n",
            "Epoch: [2]  [ 910/8470]  eta: 1:20:54  lr: 0.005000  loss: 0.6591 (0.7065)  loss_classifier: 0.1949 (0.2322)  loss_box_reg: 0.3502 (0.3555)  loss_objectness: 0.0277 (0.0367)  loss_rpn_box_reg: 0.0705 (0.0820)  time: 0.6403  data: 0.0159  max mem: 5151\n",
            "Epoch: [2]  [ 920/8470]  eta: 1:20:48  lr: 0.005000  loss: 0.7075 (0.7068)  loss_classifier: 0.2216 (0.2323)  loss_box_reg: 0.3542 (0.3556)  loss_objectness: 0.0263 (0.0368)  loss_rpn_box_reg: 0.0783 (0.0821)  time: 0.6379  data: 0.0153  max mem: 5151\n",
            "Epoch: [2]  [ 930/8470]  eta: 1:20:41  lr: 0.005000  loss: 0.7205 (0.7073)  loss_classifier: 0.2364 (0.2323)  loss_box_reg: 0.3493 (0.3558)  loss_objectness: 0.0389 (0.0369)  loss_rpn_box_reg: 0.0955 (0.0822)  time: 0.6400  data: 0.0166  max mem: 5151\n",
            "Epoch: [2]  [ 940/8470]  eta: 1:20:34  lr: 0.005000  loss: 0.6428 (0.7062)  loss_classifier: 0.2131 (0.2319)  loss_box_reg: 0.3218 (0.3554)  loss_objectness: 0.0321 (0.0367)  loss_rpn_box_reg: 0.0726 (0.0821)  time: 0.6379  data: 0.0164  max mem: 5151\n",
            "Epoch: [2]  [ 950/8470]  eta: 1:20:28  lr: 0.005000  loss: 0.5931 (0.7057)  loss_classifier: 0.1893 (0.2317)  loss_box_reg: 0.3215 (0.3551)  loss_objectness: 0.0229 (0.0367)  loss_rpn_box_reg: 0.0736 (0.0822)  time: 0.6373  data: 0.0154  max mem: 5151\n",
            "Epoch: [2]  [ 960/8470]  eta: 1:20:21  lr: 0.005000  loss: 0.6467 (0.7050)  loss_classifier: 0.2055 (0.2315)  loss_box_reg: 0.3259 (0.3548)  loss_objectness: 0.0287 (0.0366)  loss_rpn_box_reg: 0.0853 (0.0821)  time: 0.6413  data: 0.0159  max mem: 5151\n",
            "Epoch: [2]  [ 970/8470]  eta: 1:20:14  lr: 0.005000  loss: 0.6403 (0.7052)  loss_classifier: 0.2110 (0.2317)  loss_box_reg: 0.3259 (0.3549)  loss_objectness: 0.0285 (0.0366)  loss_rpn_box_reg: 0.0718 (0.0820)  time: 0.6392  data: 0.0165  max mem: 5151\n",
            "Epoch: [2]  [ 980/8470]  eta: 1:20:08  lr: 0.005000  loss: 0.6841 (0.7055)  loss_classifier: 0.2404 (0.2319)  loss_box_reg: 0.3741 (0.3551)  loss_objectness: 0.0261 (0.0366)  loss_rpn_box_reg: 0.0709 (0.0819)  time: 0.6380  data: 0.0169  max mem: 5151\n",
            "Epoch: [2]  [ 990/8470]  eta: 1:20:02  lr: 0.005000  loss: 0.7008 (0.7056)  loss_classifier: 0.2388 (0.2320)  loss_box_reg: 0.3725 (0.3551)  loss_objectness: 0.0259 (0.0366)  loss_rpn_box_reg: 0.0709 (0.0820)  time: 0.6460  data: 0.0185  max mem: 5151\n",
            "Epoch: [2]  [1000/8470]  eta: 1:19:55  lr: 0.005000  loss: 0.6785 (0.7053)  loss_classifier: 0.2145 (0.2319)  loss_box_reg: 0.3308 (0.3550)  loss_objectness: 0.0330 (0.0366)  loss_rpn_box_reg: 0.0742 (0.0819)  time: 0.6433  data: 0.0172  max mem: 5151\n",
            "Epoch: [2]  [1010/8470]  eta: 1:19:48  lr: 0.005000  loss: 0.5889 (0.7050)  loss_classifier: 0.1959 (0.2317)  loss_box_reg: 0.3308 (0.3551)  loss_objectness: 0.0322 (0.0366)  loss_rpn_box_reg: 0.0667 (0.0817)  time: 0.6349  data: 0.0140  max mem: 5151\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}