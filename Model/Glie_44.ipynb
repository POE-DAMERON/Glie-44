{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glie_44.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POE-DAMERON/Glie-44/blob/Pytorch/Model/Glie_44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPx23mf0avpR"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from os import path, listdir\n",
        "from pathlib import Path\n",
        "import cv2 as cv\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import io\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "  Downloads Data from the VisDrone dataset.\n",
        "  Input is which dataset to download:\n",
        "    - 0 is the training dataset\n",
        "    - 1 is the developper testing dataset\n",
        "    - 2 is the actual challenge testing dataset\n",
        "    - 3 is the val dataset\n",
        "'''\n",
        "\n",
        "def initialize_training(file = 0):\n",
        "  !git clone https://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@github.com/POE-DAMERON/Glie-44.git\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  \n",
        "  if file == 1:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-dev.zip\n",
        "  elif file == 2:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-test-challenge.zip\n",
        "  elif file == 3:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-val.zip\n",
        "  else:\n",
        "    !unzip /content/drive/MyDrive/VisDrone2019-MOT-train.zip\n",
        "\n",
        "initialize_training()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qfqt22wD3GD",
        "outputId": "8aeb76e4-a37a-4a50-a93c-be4ed140ea9d"
      },
      "source": [
        "!git init"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwwkaAuJavpY"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  @staticmethod\n",
        "  def add_blocks(pred, draw, height, width, font_path, precision):\n",
        "    boxes = pred['detection_boxes'].numpy()[0]\n",
        "    classes = pred['detection_classes'].numpy()[0]\n",
        "\n",
        "    boxes[:, 0] *= height\n",
        "    boxes[:, 1] *= width\n",
        "    boxes[:, 2] *= height\n",
        "    boxes[:, 3] *= width\n",
        "    \n",
        "    for i in range(len(boxes)):\n",
        "      if pred['detection_scores'].numpy()[0][i] > precision:\n",
        "        draw.rectangle(Utils.prepare_coords(boxes[i]), outline = Utils.which_color(classes[i]), width = 3)\n",
        "        draw.text((boxes[i][1], boxes[i][0]),str(classes[i])[:-2], fill=(255,255,255), stroke_fill= (0,0,0,255), stroke_width = 2, font= ImageFont.truetype(font_path, 20))\n",
        "\n",
        "  @staticmethod\n",
        "  def which_color(class_id):\n",
        "    color_value = int(class_id) * 9\n",
        "    return (min(color_value, 255), max(min(color_value - 255, 255),0),max(min(color_value - 256 * 2 - 1, 255),0))\n",
        "\n",
        "  @staticmethod\n",
        "  def prepare_coords(array):\n",
        "    return (array[1], array[0], array[3], array[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7cbjDkH6X9"
      },
      "source": [
        "class Glie_44():\n",
        "\n",
        "  def __init__(self, precision = .2, font_path = Path().absolute().joinpath('Glie-44/Model/Data/fonts/Roboto-Regular.ttf')):\n",
        "    self._precision = precision\n",
        "    self._font_path = str(font_path)\n",
        "\n",
        "\n",
        "  def set_font_path(self, font_path):\n",
        "    self._font_path = font_path\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "    self._model = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
        "\n",
        "  def set_model(self,model):\n",
        "    self._model = model\n",
        "\n",
        "  def get_image_data(self, image_path):\n",
        "    image_data = plt.imread(image_path)\n",
        "    return np.array(image_data)\n",
        "\n",
        "  def pred(self, image_data):\n",
        "    return self._model(image_data)\n",
        "\n",
        "  def pred_on_image_path(self, image_path):\n",
        "    image_data = np.array([self.get_image_data(image_path)])\n",
        "    return self.pred(image_data)\n",
        "  \n",
        "  def run_on_image(self, image_data):\n",
        "    pred = self.pred(np.array([image_data]))\n",
        "\n",
        "    image = Image.fromarray(image_data)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    Utils.add_blocks(pred,draw, image.size[1], image.size[0], self._font_path, self._precision)\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "  def run_on_image_with_path(self, image_path):\n",
        "    pred = self.pred_on_image_path(image_path)\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    Utils.add_blocks(pred,draw, image.size[1], image.size[0], self._font_path, self._precision)\n",
        "\n",
        "    return np.array(image)\n",
        "\n",
        "  # Takes a folder path where the images should be in chronological order to \n",
        "  # detect and compile in a video\n",
        "\n",
        "  def run_on_folder(self, folder_path = Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v'), output_folder = Path().absolute().joinpath('Glie-44/Model/outputs'), framerate = 20):\n",
        "\n",
        "    video_frames = sorted(listdir(folder_path), key=lambda x: x.lstrip(\"_\"))\n",
        "    s = Image.open(Path(folder_path).joinpath(video_frames[0])).size\n",
        "\n",
        "    output_path = path.basename(folder_path) + '.avi'\n",
        "    fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "    writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "\n",
        "    sample = video_frames\n",
        "    total_frames = len(sample)\n",
        "    for i in range(total_frames):\n",
        "      #print(i + 1, '/', total_frames)\n",
        "      result = self.run_on_image_with_path(Path(folder_path).joinpath(sample[i]))\n",
        "      #print(sample[i], ':', result.shape)\n",
        "      writer.write(cv.cvtColor(result,cv.COLOR_RGB2BGR))\n",
        "\n",
        "    writer.release()\n",
        "\n",
        "  # Takes a video file to detect objects\n",
        "\n",
        "  def run_on_video(self, video_path, output_folder = Path().absolute().joinpath('Glie-44/Model/outputs'), framerate = 24):\n",
        "    cap = cv.VideoCapture(video_path)\n",
        "\n",
        "    output_path = Path(video_path).stem + '_output.avi'\n",
        "\n",
        "    if (cap.isOpened()):\n",
        "      s = (int(cap.get(3)),int(cap.get(4)))\n",
        "      fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "      writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      while ret == True:\n",
        "        result = self.run_on_image(frame)\n",
        "          \n",
        "        writer.write(result)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "  #  takes a video stream and outputs a video stream with the boxes around the\n",
        "  #  detected objects\n",
        "\n",
        "  def run_on_stream():\n",
        "    pass\n",
        "\n",
        "  def build_video_from_directory(self, folder_path = Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v'), output_folder = Path().absolute(), framerate = 24):\n",
        "    video_frames = sorted(listdir(folder_path), key=lambda x: x.lstrip(\"_\"))\n",
        "    s = Image.open(Path(folder_path).joinpath(video_frames[0])).size\n",
        "\n",
        "    output_path = 'Video_test.avi'\n",
        "    fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
        "    writer = cv.VideoWriter(str(output_folder) + '/' + output_path, fourcc, framerate, s)\n",
        "\n",
        "    sample = video_frames\n",
        "    total_frames = len(sample)\n",
        "    for i in range(total_frames):\n",
        "      print(i + 1, '/', total_frames)\n",
        "      result = self.get_image_data(Path(folder_path).joinpath(sample[i]))#np.array(Image.open(Path(folder_path).joinpath(sample[i])))\n",
        "      #print(sample[i], ':', result.shape)\n",
        "      writer.write(cv.cvtColor(result,cv.COLOR_RGB2BGR))\n",
        "\n",
        "    writer.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dTQKkbavpZ"
      },
      "source": [
        "#model2 = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
        "model2 = hub.load(\"https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtmL4UJoavpa"
      },
      "source": [
        "glie = Glie_44()\n",
        "glie.set_model(model2)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#    TO TEST THE RUN_ON_IMAGE\n",
        "\n",
        "pred_image = glie.run_on_image_with_path('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg')\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(pred_image)\n",
        "\n",
        "pred = glie.pred_on_image_path('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg')\n",
        "pred\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "glie.run_on_video(video_path='Video_test.avi',output_folder=Path().absolute())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFCDWKdWavpa"
      },
      "source": [
        "#test = hub.KerasLayer(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\", trainable=True)\n",
        "test = hub.KerasLayer(\"https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1\", trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVCO4OB_b9fC",
        "outputId": "efb4dd22-a722-4ec6-9136-3945f3c8a436"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = np.array(Image.open('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg'))\n",
        "shape = x.shape\n",
        "x = np.array([])\n",
        "\n",
        "video_frames = sorted(listdir(folder_path), key=lambda x: x.lstrip(\"_\"))\n",
        "total_frames = len(video_frames)\n",
        "for i in range(total_frames):\n",
        "  x[i] = Image.open(Path().absolute().joinpath('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_00000_v').joinpath(video_frames[i]))\n",
        "\n",
        "initial_layer = tf.keras.Input(shape=shape, name=\"initial_layer\",dtype=tf.uint8)\n",
        "layer = test(initial_layer)['detection_scores']# ou ['detection_boxes']\n",
        "print(layer)\n",
        "#layer = tf.keras.layers.Flatten()(initial_layer)\n",
        "layer = tf.keras.layers.Flatten()(layer)\n",
        "layer = tf.keras.layers.Dense(100, activation='relu',)(layer)\n",
        "last_layer = tf.keras.layers.Dense(10, activation='softmax')(layer)\n",
        "\n",
        "model = tf.keras.Model(initial_layer, last_layer)\n",
        "\n",
        "\n",
        "model.compile(optimizer=\"adam\", metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "model.fit(x=x, y=np.ones((1,10)), batch_size=1, epochs = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(1, 100), dtype=tf.float32, name=None), name='keras_layer/StatefulPartitionedCall:2', description=\"created by layer 'keras_layer'\")\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "initial_layer (InputLayer)   [(None, 756, 1344, 3)]    0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     {'detection_boxes': (1, 1 0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (1, 100)                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, 100)                  10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, 10)                   1010      \n",
            "=================================================================\n",
            "Total params: 11,110\n",
            "Trainable params: 11,110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 10s 10s/step - loss: 23.3216 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1a89490f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd6FAKVtYPk_"
      },
      "source": [
        "class MultipleOutputLayer():\n",
        "  def __init__(self,layer = tf.keras.Input(shape=()), output_key = ''):\n",
        "    self._layer = layer\n",
        "    self._output_key = output_key\n",
        "\n",
        "  def set_output_key(self, key):\n",
        "    self._output_key = key\n",
        "\n",
        "  def get_output_key(self):\n",
        "    return self._output_key\n",
        "  \n",
        "  def set_layer(self, layer):\n",
        "    self._layer = layer\n",
        "\n",
        "  def get_layer(self):\n",
        "    return self._layer\n",
        "\n",
        "class Architecture():\n",
        "\n",
        "  def __init__(self, initial_layer = tf.keras.Input(shape=()), output_layer = tf.keras.layers.Dense(0)):\n",
        "    self._initial_layer = initial_layer\n",
        "    self._mid_layers = []\n",
        "    self._output_layer = output_layer\n",
        "\n",
        "  def set_initial_layer(self, layer):\n",
        "    self._initial_layer = layer\n",
        "\n",
        "  def get_initial_layer(self):\n",
        "    return self._initial_layer\n",
        "\n",
        "  def set_output_layer(self, layer):\n",
        "    self._output_layer = layer\n",
        "\n",
        "  def get_output_layer(self):\n",
        "    return self._output_layer\n",
        "\n",
        "  def get_mid_layers(self):\n",
        "    return self._mid_layers\n",
        "  \n",
        "  def __getitem__(self, layer_number):\n",
        "    return self._mid_layers[layer_number]\n",
        "  \n",
        "  def __setitem__(self, layer_number, data):\n",
        "    self._mid_layers[layer_number] = data\n",
        "  \n",
        "  def compile(self):\n",
        "    try :\n",
        "      mid_layer = self._initial_layer\n",
        "      for layer in self._mid_layers:\n",
        "        if type(layer) == MultipleOutputLayer:\n",
        "          mid_layer = layer.get_layer()(mid_layer)[layer.get_output_key()]\n",
        "        else:\n",
        "          mid_layer = layer(mid_layer)\n",
        "      return (self._initial_layer, self._output_layer(mid_layer))\n",
        "    except Exception as e:\n",
        "      print(f'Sorry. Something went wrong:\\n{e}')\n",
        "      pass\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, compiled_arc):\n",
        "    super().__init__(compiled_arc[0], compiled_arc[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecNkSvaP41QM",
        "outputId": "5cea128e-9981-4ca5-89ac-29ceff812be7"
      },
      "source": [
        "x = np.array(Image.open('Glie-44/Model/Data/VisDrone2019-MOT-train/sequences/uav0000013_01073_v/0000001.jpg'))\n",
        "shape = x.shape\n",
        "\n",
        "arc = Architecture()\n",
        "arc.set_initial_layer(tf.keras.Input(shape=shape, name=\"initial_layer\",dtype=tf.uint8))\n",
        "arc.get_mid_layers().append(MultipleOutputLayer(test, 'detection_scores'))\n",
        "arc.get_mid_layers().append(tf.keras.layers.Flatten())\n",
        "arc.set_output_layer(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "res = arc.compile()\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<KerasTensor: shape=(None, 756, 1344, 3) dtype=uint8 (created by layer 'initial_layer')>,\n",
              " <KerasTensor: shape=(1, 10) dtype=float32 (created by layer 'dense_11')>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDUCwkZR5OWD",
        "outputId": "f33a5260-b044-4d60-d163-cbdfbe31f4e8"
      },
      "source": [
        "model_test = Model(res)\n",
        "model_test.summary()\n",
        "model_test.compile(optimizer=\"adam\", metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "model_test.predict(np.array([x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "initial_layer (InputLayer)   [(None, 756, 1344, 3)]    0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     {'detection_boxes': (1, 1 0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (1, 100)                  0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (1, 10)                   1010      \n",
            "=================================================================\n",
            "Total params: 1,010\n",
            "Trainable params: 1,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09631571, 0.03675139, 0.13114575, 0.13017394, 0.09900831,\n",
              "        0.10070857, 0.17002572, 0.06272071, 0.08706453, 0.08608536]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWgZe8DQZS4"
      },
      "source": [
        "**New Model using Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EcvQ0gWTmO"
      },
      "source": [
        "Main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saxy2LvOmdbn",
        "outputId": "c72b6bd1-2d9b-4ac2-8bef-525565fe0453"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKOtVOkjWSps"
      },
      "source": [
        "import torch\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import os\n",
        "import pandas as pd\n",
        "import transforms as T\n",
        "from pathlib import Path\n",
        "import csv\n",
        "\n",
        "def get_results(evaluator):\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = buffer = io.StringIO()\n",
        "\n",
        "  result = str(evaluator.coco_eval)\n",
        "  test = evaluator.coco_eval.items()\n",
        "  for iou_type, coco_eval in test:\n",
        "    print(\"IoU metric: {}\".format(iou_type))\n",
        "    try:\n",
        "      print(coco_eval)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  sys.stdout = old_stdout\n",
        "\n",
        "  return buffer.getvalue()\n",
        "\n",
        "def add_to_record(arguments, output, is_saved, path_to_saved_model = ''):\n",
        "  with open('Glie-44/Model/outputs/training.csv', 'a', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    #writer.writerow(['Saved', 'path_to_saved_model', 'number_of_epochs', 'batch_size', 'optimizer', 'learning_rate', 'weight_decay', 'momentum', 'lr_scheduler_step_size', 'lr_scheduler_gamma', 'output'])\n",
        "    writer.writerow([is_saved, path_to_saved_model, arguments['number_of_epochs'], arguments['batch_size'], arguments['optimizer'], arguments['lr'], arguments['weight_decay'], arguments['momentum'], arguments['lr_scheduler_step_size'], arguments['lr_scheduler_gamma'], output])\n",
        "\n",
        "def save_model(model, evaluation, arguments, path):\n",
        "  \"\"\"\n",
        "      saves the model and adds to the csv records\n",
        "\n",
        "      2 methods :\n",
        "        - saving the whole model\n",
        "        - saving the weights and info\n",
        "  \"\"\"\n",
        "  torch.save(model,path) \n",
        "\n",
        "  #torch.save(model.state_dict(), path)\n",
        "  add_to_record(arguments = arguments, output = get_results(evaluation), is_saved = True, path_to_saved_model = path)\n",
        "  \n",
        "def load_model(path):\n",
        "\n",
        "  model = torch.load(path)\n",
        "\n",
        "  \"\"\"\n",
        "    model = Model()\n",
        "    model.load_state_dict(torch.load(path))\n",
        "  \"\"\"\n",
        "\n",
        "  return model\n",
        "\n",
        "def get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma):\n",
        "  \n",
        "  arguments = {}\n",
        "\n",
        "  arguments['train_percentage'] = train_percentage\n",
        "  if (optimizer == 'adam'):\n",
        "    arguments['optimizer'] = optimizer\n",
        "    arguments['momentum'] = ''\n",
        "  else:\n",
        "    arguments['optimizer'] = 'sgd'\n",
        "    arguments['momentum'] = momentum\n",
        "  arguments['lr'] = lr\n",
        "  arguments['weight_decay'] = weight_decay\n",
        "  arguments['lr_scheduler_step_size'] = step_size\n",
        "  arguments['lr_scheduler_gamma'] = gamma\n",
        "  arguments['number_of_epochs'] = epochs\n",
        "  arguments['batch_size'] = batch_size\n",
        "\n",
        "  return arguments\n",
        "\n",
        "def train(path_to_saved_model = '',train_percentage = .8, batch_size = 2,\n",
        "          epochs = 10, optimizer='sgd', lr = 0.005, momentum = 0.9,\n",
        "          weight_decay= 0.0005, step_size = 3, gamma = 0.1):\n",
        "  \n",
        "  arguments = get_arguments(train_percentage, epochs, batch_size, optimizer,\n",
        "                            lr, momentum, weight_decay, step_size, gamma)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  x = VisDroneDataset(Path().absolute().joinpath('VisDrone2019-MOT-train'), Utils.to_tensor())\n",
        "  X = x[0]\n",
        "\n",
        "  train_size = int(len(X) * train_percentage)\n",
        "\n",
        "  x_train, x_test = torch.utils.data.random_split(X, [train_size, len(X) - train_size])\n",
        "\n",
        "  data_loader = torch.utils.data.DataLoader(\n",
        "        x_train, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "  \n",
        "  data_loader_test = torch.utils.data.DataLoader(\n",
        "    x_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "  \n",
        "  if str(path_to_saved_model) == '':\n",
        "    model = build_model(12)\n",
        "  else:\n",
        "    model = load_model(path_to_saved_model)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  if (optimizer == 'adam'):\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optim = torch.optim.SGD(params, lr=lr,\n",
        "                            momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optim,\n",
        "                                                   step_size=step_size,\n",
        "                                                   gamma=gamma)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "        train_one_epoch(model, optim, data_loader, device, epoch, print_freq=10)\n",
        "        lr_scheduler.step()\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "  \n",
        "  return model, evaluate(model, data_loader_test, device=device), arguments"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyCvAuqO12L1"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "def build_model(num_classes = 12):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "  backbone.out_channels = 1280\n",
        "\n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                  output_size=7,\n",
        "                                                  sampling_ratio=2)\n",
        "\n",
        "  return FasterRCNN(backbone,\n",
        "                    num_classes=num_classes,\n",
        "                    rpn_anchor_generator=anchor_generator,\n",
        "                    box_roi_pool=roi_pooler)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLmxWKCkRRan"
      },
      "source": [
        "class VisDroneVideo(object):\n",
        "    def __init__(self ,root, target_path, preprocessing = None):\n",
        "        self.root = str(root)\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        self.imgs = sorted(listdir(Path(root)), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.target = target_path\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = Path(self.root).joinpath(self.imgs[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        video_targets = Utils.read_txt_visdrone(self.target)\n",
        "        image_targets = self.clean_targets(video_targets, idx)\n",
        "        image_targets[\"is_crowd\"] = 0\n",
        "        boxes = image_targets[[\"bbox_left\", \"bbox_top\", \"right\", \"bottom\"]]\n",
        "\n",
        "        boxes = boxes.astype('float32')\n",
        "\n",
        "        boxes = torch.as_tensor(boxes.values, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(image_targets.object_category.astype('int64').values, dtype=torch.int64)\n",
        "        crowd = torch.as_tensor(image_targets.is_crowd.values, dtype=torch.int64)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = crowd\n",
        "\n",
        "        if self.preprocessing is not None:\n",
        "            img, target = self.preprocessing(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def clean_targets(self, targets, idx):\n",
        "\n",
        "        targets = self.targetsToDataframe(targets)\n",
        "        targets = targets[(targets.object_category != \"0\")]\n",
        "        targets = targets[(targets.frame_index == str(idx+1))]\n",
        "\n",
        "        targets.bbox_top = targets.bbox_top.astype('float32')\n",
        "        targets.bbox_height = targets.bbox_height.astype('float32')\n",
        "        targets.bbox_left = targets.bbox_left.astype('float32')\n",
        "        targets.bbox_width = targets.bbox_width.astype('float32')\n",
        "\n",
        "        targets[\"bottom\"] = targets.bbox_top + targets.bbox_height\n",
        "        targets[\"right\"] = targets.bbox_left + targets.bbox_width\n",
        "\n",
        "        return targets\n",
        "    \n",
        "    def targetsToDataframe(self, array):\n",
        "        columns = [\n",
        "          \"frame_index\",\n",
        "          \"target_id\",\n",
        "          \"bbox_left\",\n",
        "          \"bbox_top\",\n",
        "          \"bbox_width\",\n",
        "          \"bbox_height\",\n",
        "          \"score\",\n",
        "          \"object_category\",\n",
        "          \"truncation\",\n",
        "          \"oclusion\"\n",
        "        ]\n",
        "        return pd.DataFrame(data=array,columns=columns)\n",
        "\n",
        "class VisDroneDataset(object):\n",
        "    def __init__(self, root, preprocessing = None):\n",
        "        self.root = root\n",
        "        self.preprocessing = preprocessing\n",
        "        \n",
        "        self.videos = sorted(listdir(Path(root).joinpath(\"sequences\")), key=lambda x: x.lstrip(\"_\"))\n",
        "        self.targets = sorted(listdir(Path(root).joinpath(\"annotations\")), key=lambda x: x.lstrip(\"_\"))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      return VisDroneVideo(Path(self.root).joinpath(\"sequences\").joinpath(self.videos[idx]),Path(self.root).joinpath(\"annotations\").joinpath(self.targets[idx]), self.preprocessing)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    '''\n",
        "        Concatenates the images of all videos into a large VisDroneVideo class\n",
        "        Allows the model to train on a broader sample of data\n",
        "    \n",
        "\n",
        "    def all_images(self):\n",
        "        large_dataset = VisDroneVideo()\n",
        "\n",
        "        for i in self:\n",
        "          for j in i[0]:\n",
        "            print(j)\n",
        "\n",
        "    '''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XhaZ_CZ6Z1"
      },
      "source": [
        "Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKM3TFaZ6M8"
      },
      "source": [
        "class Utils():\n",
        "\n",
        "  @staticmethod\n",
        "  def to_tensor():\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "  @staticmethod\n",
        "  def read_txt_visdrone(path):\n",
        "      \"\"\"\n",
        "          Input: Path of the txt file with annotations as in the VisDrone dataset\\n\n",
        "          Output: A numpy array containing the information for bounding boxes\n",
        "      \"\"\"\n",
        "      lines = []\n",
        "      with open(path) as f:\n",
        "          lines = f.readlines()\n",
        "          f.close()\n",
        "      df = []\n",
        "      for x in lines:\n",
        "          splitLine = x.split(\",\")\n",
        "          splitLine[-1] = splitLine[-1].split(\"\\n\")[0]\n",
        "          df.append(splitLine)\n",
        "      return df\n",
        "      \n",
        "  @staticmethod\n",
        "  def bottom_right(left,top,width,height):\n",
        "      \"\"\"\n",
        "          Input: co-ordinates for the top left corner of a bounding box and its width and height\\n\n",
        "          Output: co-ordinates for the bottom right corner\n",
        "      \"\"\"\n",
        "      bottom = top + height\n",
        "      right = left + width\n",
        "      return right, bottom"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn4oKgG2g3nu",
        "outputId": "60bc7e7b-cd43-4785-da4e-c899f8a58cee"
      },
      "source": [
        "eval = train(path_to_saved_model= 'model.pth', epochs = 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [  0/108]  eta: 0:02:00  lr: 0.000052  loss: 0.9397 (0.9397)  loss_classifier: 0.3728 (0.3728)  loss_box_reg: 0.4417 (0.4417)  loss_objectness: 0.0773 (0.0773)  loss_rpn_box_reg: 0.0479 (0.0479)  time: 1.1154  data: 0.6201  max mem: 3258\n",
            "Epoch: [0]  [ 10/108]  eta: 0:00:44  lr: 0.000519  loss: 0.8990 (0.9088)  loss_classifier: 0.3383 (0.3415)  loss_box_reg: 0.4536 (0.4565)  loss_objectness: 0.0678 (0.0725)  loss_rpn_box_reg: 0.0363 (0.0383)  time: 0.4527  data: 0.0647  max mem: 3882\n",
            "Epoch: [0]  [ 20/108]  eta: 0:00:37  lr: 0.000985  loss: 0.9154 (0.9236)  loss_classifier: 0.3442 (0.3500)  loss_box_reg: 0.4633 (0.4623)  loss_objectness: 0.0694 (0.0726)  loss_rpn_box_reg: 0.0363 (0.0387)  time: 0.3875  data: 0.0093  max mem: 3882\n",
            "Epoch: [0]  [ 30/108]  eta: 0:00:32  lr: 0.001452  loss: 0.9548 (0.9229)  loss_classifier: 0.3489 (0.3491)  loss_box_reg: 0.4829 (0.4622)  loss_objectness: 0.0749 (0.0738)  loss_rpn_box_reg: 0.0363 (0.0380)  time: 0.3906  data: 0.0093  max mem: 3882\n",
            "Epoch: [0]  [ 40/108]  eta: 0:00:27  lr: 0.001919  loss: 0.9582 (0.9344)  loss_classifier: 0.3466 (0.3521)  loss_box_reg: 0.4880 (0.4687)  loss_objectness: 0.0775 (0.0753)  loss_rpn_box_reg: 0.0363 (0.0383)  time: 0.3923  data: 0.0095  max mem: 3882\n",
            "Epoch: [0]  [ 50/108]  eta: 0:00:23  lr: 0.002386  loss: 0.9300 (0.9306)  loss_classifier: 0.3376 (0.3503)  loss_box_reg: 0.4764 (0.4687)  loss_objectness: 0.0757 (0.0739)  loss_rpn_box_reg: 0.0353 (0.0378)  time: 0.3926  data: 0.0095  max mem: 3882\n",
            "Epoch: [0]  [ 60/108]  eta: 0:00:19  lr: 0.002853  loss: 0.9121 (0.9309)  loss_classifier: 0.3225 (0.3455)  loss_box_reg: 0.4761 (0.4732)  loss_objectness: 0.0716 (0.0742)  loss_rpn_box_reg: 0.0369 (0.0380)  time: 0.3937  data: 0.0095  max mem: 3882\n",
            "Epoch: [0]  [ 70/108]  eta: 0:00:15  lr: 0.003319  loss: 0.9272 (0.9296)  loss_classifier: 0.3178 (0.3414)  loss_box_reg: 0.5034 (0.4772)  loss_objectness: 0.0681 (0.0730)  loss_rpn_box_reg: 0.0396 (0.0380)  time: 0.3985  data: 0.0094  max mem: 3882\n",
            "Epoch: [0]  [ 80/108]  eta: 0:00:11  lr: 0.003786  loss: 0.9304 (0.9287)  loss_classifier: 0.3158 (0.3360)  loss_box_reg: 0.5034 (0.4821)  loss_objectness: 0.0675 (0.0724)  loss_rpn_box_reg: 0.0393 (0.0382)  time: 0.4044  data: 0.0100  max mem: 3882\n",
            "Epoch: [0]  [ 90/108]  eta: 0:00:07  lr: 0.004253  loss: 0.8768 (0.9220)  loss_classifier: 0.2807 (0.3296)  loss_box_reg: 0.4996 (0.4841)  loss_objectness: 0.0604 (0.0707)  loss_rpn_box_reg: 0.0361 (0.0376)  time: 0.4061  data: 0.0104  max mem: 3882\n",
            "Epoch: [0]  [100/108]  eta: 0:00:03  lr: 0.004720  loss: 0.8870 (0.9251)  loss_classifier: 0.2808 (0.3280)  loss_box_reg: 0.5130 (0.4897)  loss_objectness: 0.0604 (0.0698)  loss_rpn_box_reg: 0.0339 (0.0376)  time: 0.4060  data: 0.0102  max mem: 3882\n",
            "Epoch: [0]  [107/108]  eta: 0:00:00  lr: 0.005000  loss: 0.9315 (0.9248)  loss_classifier: 0.3023 (0.3256)  loss_box_reg: 0.5424 (0.4926)  loss_objectness: 0.0611 (0.0691)  loss_rpn_box_reg: 0.0356 (0.0375)  time: 0.3977  data: 0.0096  max mem: 3882\n",
            "Epoch: [0] Total time: 0:00:43 (0.4032 s / it)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/54]  eta: 0:00:22  model_time: 0.1149 (0.1149)  evaluator_time: 0.0362 (0.0362)  time: 0.4108  data: 0.2551  max mem: 3882\n",
            "Test:  [53/54]  eta: 0:00:00  model_time: 0.0614 (0.0627)  evaluator_time: 0.0343 (0.0378)  time: 0.1158  data: 0.0124  max mem: 3882\n",
            "Test: Total time: 0:00:06 (0.1181 s / it)\n",
            "Averaged stats: model_time: 0.0614 (0.0627)  evaluator_time: 0.0343 (0.0378)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.120\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.253\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.112\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.098\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.241\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/54]  eta: 0:00:23  model_time: 0.1044 (0.1044)  evaluator_time: 0.0378 (0.0378)  time: 0.4344  data: 0.2875  max mem: 3882\n",
            "Test:  [53/54]  eta: 0:00:00  model_time: 0.0601 (0.0619)  evaluator_time: 0.0357 (0.0413)  time: 0.1059  data: 0.0065  max mem: 3882\n",
            "Test: Total time: 0:00:06 (0.1196 s / it)\n",
            "Averaged stats: model_time: 0.0601 (0.0619)  evaluator_time: 0.0357 (0.0413)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.08s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.120\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.253\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.112\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.098\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.241\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiC4psA5EMAi"
      },
      "source": [
        "add_to_record(arguments = eval[2], output = get_results(eval[1]),is_saved = False)\n",
        "#save_model(eval[0], eval[1], eval[2], 'model.pth')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCt_fJY74uI",
        "outputId": "b6a813e4-c3b4-401b-f16d-b8d0cc367b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!git init\n",
        "!git config user.email = 'vincent.briat@hotmail.fr'\n",
        "!git config --global user.name = 'Vincent'\n",
        "!git add Glie-44/Model/outputs/training.csv\n",
        "!git status\n",
        "!git commit -m 'training update'\n",
        "!git push origin pytorch"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mGlie-44/.gitignore.txt\u001b[m\n",
            "\t\u001b[31mGlie-44/Copy_of_Glie_44.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/.ipynb_checkpoints/\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Data/\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Glie_44.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Image.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Video_test.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Video_test_output.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/make_tfrecord.py\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/model.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/outputs/uav0000013_00000_v.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/outputs/uav0000013_01073_v.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/utils.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/utils.py\u001b[m\n",
            "\t\u001b[31mVisDrone2019-MOT-train/\u001b[m\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\t\u001b[31mcoco_eval.py\u001b[m\n",
            "\t\u001b[31mcoco_utils.py\u001b[m\n",
            "\t\u001b[31mdrive/\u001b[m\n",
            "\t\u001b[31mengine.py\u001b[m\n",
            "\t\u001b[31mmodel.pth\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\t\u001b[31mtransforms.py\u001b[m\n",
            "\t\u001b[31mutils.py\u001b[m\n",
            "\t\u001b[31mvision/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "On branch master\n",
            "Untracked files:\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mGlie-44/.gitignore.txt\u001b[m\n",
            "\t\u001b[31mGlie-44/Copy_of_Glie_44.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/.ipynb_checkpoints/\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Data/\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Glie_44.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Image.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Video_test.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/Video_test_output.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/make_tfrecord.py\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/model.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/outputs/uav0000013_00000_v.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/outputs/uav0000013_01073_v.avi\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/utils.ipynb\u001b[m\n",
            "\t\u001b[31mGlie-44/Model/utils.py\u001b[m\n",
            "\t\u001b[31mVisDrone2019-MOT-train/\u001b[m\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\t\u001b[31mcoco_eval.py\u001b[m\n",
            "\t\u001b[31mcoco_utils.py\u001b[m\n",
            "\t\u001b[31mdrive/\u001b[m\n",
            "\t\u001b[31mengine.py\u001b[m\n",
            "\t\u001b[31mmodel.pth\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\t\u001b[31mtransforms.py\u001b[m\n",
            "\t\u001b[31mutils.py\u001b[m\n",
            "\t\u001b[31mvision/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present\n",
            "error: src refspec pytorch does not match any.\n",
            "error: failed to push some refs to 'origin'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX3MSKVqgxex"
      },
      "source": [
        "print('Hello')"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}